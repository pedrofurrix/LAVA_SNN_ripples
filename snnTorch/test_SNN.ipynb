{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import snntorch as snn\n",
    "from snntorch import surrogate\n",
    "from snntorch import backprop\n",
    "from snntorch import functional as SF\n",
    "from snntorch import utils\n",
    "from snntorch import spikeplot as splt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader arguments\n",
    "batch_size = 128\n",
    "data_path=r\"C:\\Users\\NCN\\Documents\\PedroFelix\\LAVA_SNN_ripples\\extract_Nripples\\train_pedro\\n_dataset\\up_down\"\n",
    "bandpass=[100,250]\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives = np.load(os.path.join(data_path,f\"ntrue_positives_{bandpass[0]}_{bandpass[1]}Hz.npy\"))\n",
    "negatives = np.load(os.path.join(data_path,f\"ntrue_negatives_{bandpass[0]}_{bandpass[1]}Hz.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives_tensor = torch.tensor(positives, dtype=torch.float32)\n",
    "negatives_tensor = torch.tensor(negatives, dtype=torch.float32)\n",
    "print(positives_tensor.shape, negatives_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset,random_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Combine the data (positive and negative) into one dataset\n",
    "X = torch.cat((positives_tensor, negatives_tensor), dim=0)\n",
    "\n",
    "# Fixed version:\n",
    "y = torch.cat((torch.ones(positives_tensor.shape[0], dtype=torch.long),  # Note dtype\n",
    "               torch.zeros(negatives_tensor.shape[0], dtype=torch.long)), dim=0)\n",
    "\n",
    "# Create a TensorDataset\n",
    "dataset = TensorDataset(X, y)\n",
    "\n",
    "# Define split ratios\n",
    "test_ratio = 0.1  # 10% for testing\n",
    "train_ratio = 1 - test_ratio\n",
    "\n",
    "# Calculate split sizes\n",
    "num_samples = len(dataset)\n",
    "train_size = int(train_ratio * num_samples)\n",
    "test_size = num_samples - train_size\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, test_dataset = random_split(\n",
    "    dataset, \n",
    "    [train_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(42)  # for reproducibility\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,  # important for training\n",
    "    drop_last=True  # drops last incomplete batch\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False,  # no need to shuffle test data\n",
    "    drop_last=False  # we want all test samples\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Define the Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neuron and simulation parameters\n",
    "spike_grad = surrogate.fast_sigmoid(slope=25)\n",
    "beta = 0.5\n",
    "num_steps = positives.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Initialize Network\n",
    "# Note: the following code-block simulates over one single time-step, and requires a separate for-loop over time.\n",
    "input_size=2\n",
    "num_classes=2\n",
    "net = nn.Sequential(nn.Linear(input_size, 256),\n",
    "                    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True),\n",
    "                    nn.Linear(256, 64),\n",
    "                    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True),\n",
    "                    nn.Linear(64, num_classes),\n",
    "                    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True)\n",
    "                    ).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, targets = next(iter(train_loader))\n",
    "data = data.to(device)\n",
    "targets = targets.to(device)\n",
    "\n",
    "for step in range(num_steps):\n",
    "    current_data = data[:, :, step]\n",
    "    spk_out, mem_out = net(current_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(net, num_steps, data):\n",
    "  mem_rec = []\n",
    "  spk_rec = []\n",
    "  utils.reset(net)  # resets hidden states for all LIF neurons in net\n",
    "\n",
    "  for step in range(num_steps):\n",
    "      curr = data[:, :, step]\n",
    "      spk_out, mem_out = net(curr)\n",
    "      spk_rec.append(spk_out)\n",
    "      mem_rec.append(mem_out)\n",
    "  \n",
    "  return torch.stack(spk_rec, dim=0), torch.stack(mem_rec,dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spk_rec, mem_rec = forward_pass(net, num_steps, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# already imported snntorch.functional as SF \n",
    "loss_fn = SF.ce_rate_loss()\n",
    "loss_val = loss_fn(spk_rec, targets)\n",
    "\n",
    "print(f\"The loss from an untrained network is {loss_val.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = SF.accuracy_rate(spk_rec, targets)\n",
    "print(f\"The accuracy of a single batch using an untrained network is {acc*100:.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_accuracy(train_loader, net, num_steps):\n",
    "  with torch.no_grad():\n",
    "    total = 0\n",
    "    acc = 0\n",
    "    net.eval()\n",
    "    \n",
    "    train_loader = iter(train_loader)\n",
    "    for data, targets in train_loader:\n",
    "      data = data.to(device)\n",
    "      targets = targets.to(device)\n",
    "      spk_rec, _ = forward_pass(net, num_steps, data)\n",
    "\n",
    "      acc += SF.accuracy_rate(spk_rec, targets) * spk_rec.size(1)\n",
    "      total += spk_rec.size(1)\n",
    "\n",
    "  return acc/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-2, betas=(0.9, 0.999))\n",
    "num_epochs = 20\n",
    "loss_hist = []\n",
    "test_acc_hist = []\n",
    "counter = 0\n",
    "\n",
    "# Outer training loop\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Training loop\n",
    "    for data, targets in iter(train_loader):\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        net.train()\n",
    "        spk_rec, _ = forward_pass(net, num_steps, data)\n",
    "\n",
    "        # initialize the loss & sum over time\n",
    "        loss_val = loss_fn(spk_rec, targets)\n",
    "\n",
    "        # Gradient calculation + weight update\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store loss history for future plotting\n",
    "        loss_hist.append(loss_val.item())\n",
    "\n",
    "        # Test set\n",
    "        if counter % 50 == 0:\n",
    "          with torch.no_grad():\n",
    "              net.eval()\n",
    "              # Test set forward pass\n",
    "              test_acc = batch_accuracy(test_loader, net, num_steps)\n",
    "              print(f\"Iteration {counter}, Test Acc: {test_acc * 100:.2f}%\\n\")\n",
    "              test_acc_hist.append(test_acc.item())\n",
    "\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Loss\n",
    "fig = plt.figure(facecolor=\"w\")\n",
    "plt.plot(test_acc_hist)\n",
    "plt.title(\"Test Set Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Loss\n",
    "fig = plt.figure(facecolor=\"w\", figsize=(10, 5))\n",
    "plt.plot(loss_hist)\n",
    "# plt.plot(test_loss_hist)\n",
    "plt.title(\"Loss Curves\")\n",
    "plt.legend([\"Train Loss\", \"Test Loss\"])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_acc(model, test_loader, num_steps):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, targets in test_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            spk_rec, _ = forward_pass(model, num_steps, data)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            acc = SF.accuracy_rate(spk_rec, targets)\n",
    "            total_correct += acc * targets.size(0)\n",
    "            total_samples += targets.size(0)\n",
    "    \n",
    "    final_acc = total_correct / total_samples\n",
    "    print(f\"Final Test Accuracy: {final_acc*100:.2f}%\")\n",
    "    return final_acc\n",
    "\n",
    "# Run evaluation\n",
    "final_accuracy = evaluate_acc(net, test_loader, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, loader, num_steps):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, targets in loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            spk_rec, _ = forward_pass(model, num_steps, data)\n",
    "            \n",
    "            # Sum spikes over time and get predictions\n",
    "            preds = spk_rec.sum(dim=0).argmax(dim=1)  # shape: [batch_size]\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "    \n",
    "    return np.array(all_preds), np.array(all_targets)\n",
    "\n",
    "# Get predictions for entire test set\n",
    "y_pred, y_true = evaluate_model(net, test_loader, num_steps)\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample_spikes(model, sample_data, true_label, num_steps, id=0):\n",
    "    model.eval()\n",
    "    \n",
    "    # Select specific sample\n",
    "    sample_data = sample_data[id]\n",
    "    true_label = true_label[id]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        spk_rec, mem_rec = forward_pass(model, num_steps, sample_data.unsqueeze(0))  # Add batch dim\n",
    "\n",
    "    # Convert true_label to Python scalar\n",
    "    true_label = true_label.item() if torch.is_tensor(true_label) else true_label\n",
    "    \n",
    "    # Get predictions\n",
    "    total_spikes = spk_rec.sum(dim=0)  # shape: [1, 2]\n",
    "    neg_spikes = total_spikes[0, 0].item()  # Negative class\n",
    "    pos_spikes = total_spikes[0, 1].item()  # Positive class\n",
    "    \n",
    "    predicted_class = 1 if pos_spikes > neg_spikes else 0\n",
    "    true_label_str = \"Positive\" if true_label else \"Negative\"\n",
    "    pred_label_str = \"Positive\" if predicted_class else \"Negative\"\n",
    "    correct_str = \"✓ CORRECT\" if predicted_class == true_label else \"✗ WRONG\"\n",
    "\n",
    "    # Create figure with 3 subplots\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # 1. Input Spikes (assuming 2 input channels)\n",
    "    plt.subplot(3, 1, 1)\n",
    "    for i in range(2):  # For each input channel\n",
    "        spikes = sample_data[i].cpu().nonzero()[:, 0].numpy()\n",
    "        plt.eventplot(spikes, lineoffsets=i, colors=['b', 'm'][i], \n",
    "                     linewidths=2, label=f\"Input Channel {i}\")\n",
    "    plt.yticks([0, 1], [\"Channel 0\", \"Channel 1\"])\n",
    "    plt.title(f\"Input Spikes | Sample ID: {id}\")\n",
    "    plt.ylabel(\"Input Neurons\")\n",
    "    plt.legend()\n",
    "\n",
    "    # 2. Membrane Potentials\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.plot(mem_rec[:, 0, 0].cpu(), 'r', label=\"Negative Output Neuron\")\n",
    "    plt.plot(mem_rec[:, 0, 1].cpu(), 'g', label=\"Positive Output Neuron\")\n",
    "    plt.title(f\"Membrane Potentials | True: {true_label_str} | Pred: {pred_label_str} | {correct_str}\")\n",
    "    plt.ylabel(\"Potential\")\n",
    "    plt.legend()\n",
    "\n",
    "    # 3. Output Spikes\n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.eventplot(spk_rec[:, 0, 0].cpu().nonzero()[:, 0].numpy(), \n",
    "                 lineoffsets=0, colors='r', label=\"Negative\")\n",
    "    plt.eventplot(spk_rec[:, 0, 1].cpu().nonzero()[:, 0].numpy(),\n",
    "                 lineoffsets=1, colors='g', label=\"Positive\")\n",
    "    plt.yticks([0, 1], [\"Negative\", \"Positive\"])\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(\"Output Spikes\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Sample {id} | True: {true_label_str} | Predicted: {pred_label_str}\")\n",
    "    print(f\"Spike counts - Negative: {neg_spikes:.1f}, Positive: {pos_spikes:.1f}\")\n",
    "    print(f\"Classification: {correct_str}\")\n",
    "\n",
    "# Usage\n",
    "sample_data, true_label = next(iter(test_loader))\n",
    "plot_sample_spikes(net, sample_data, true_label, num_steps, id=3)  # Visualize sample 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# def plot_sample_spikes(model, test_loader, num_steps, sample_idx=None):\n",
    "#     \"\"\"Visualize spikes and membrane potentials with class-specific outputs.\"\"\"\n",
    "#     # Get sample\n",
    "#     test_data = test_loader.dataset\n",
    "#     if sample_idx is None:\n",
    "#         sample_idx = torch.randint(0, len(test_data), (1,)).item()\n",
    "    \n",
    "#     sample_data, true_label = test_data[sample_idx]\n",
    "#     sample_data = sample_data.to(device).unsqueeze(0)  # add batch dim\n",
    "\n",
    "#     # Forward pass\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         spk_rec, mem_rec = forward_pass(model, num_steps, sample_data)\n",
    "    \n",
    "#     # Prediction (which neuron spiked more)\n",
    "#     spike_counts = spk_rec.sum(0).squeeze()  # sum over time [2]\n",
    "#     predicted_label = spike_counts.argmax().item()\n",
    "#     confidence = spike_counts / spike_counts.sum()  # normalized firing rates\n",
    "\n",
    "#     # Create figure\n",
    "#     fig = plt.figure(figsize=(16, 10))\n",
    "#     gs = fig.add_gridspec(3, 2)\n",
    "    \n",
    "#     # Membrane potentials (one subplot per output neuron)\n",
    "#     ax1 = fig.add_subplot(gs[0, 0])\n",
    "#     ax1.plot(mem_rec[:, 0, 0].cpu(), 'r', label=\"Negative Class Neuron\")\n",
    "#     ax1.set_title(f\"Negative Class Output (True: {'Positive' if true_label else 'Negative'})\")\n",
    "#     ax1.set_ylabel(\"Membrane Potential\")\n",
    "#     ax1.legend()\n",
    "    \n",
    "#     ax2 = fig.add_subplot(gs[0, 1])\n",
    "#     ax2.plot(mem_rec[:, 0, 1].cpu(), 'g', label=\"Positive Class Neuron\")\n",
    "#     ax2.set_title(f\"Positive Class Output (Predicted: {'Positive' if predicted_label else 'Negative'})\")\n",
    "#     ax2.legend()\n",
    "\n",
    "#     # Output spikes (raster plot)\n",
    "#     ax3 = fig.add_subplot(gs[1, :])\n",
    "#     for i in range(2):  # for each output neuron\n",
    "#         spikes = spk_rec[:, 0, i].cpu().nonzero()[:, 0].numpy()\n",
    "#         ax3.eventplot(spikes, lineoffsets=i+1, colors=['r', 'g'][i], linewidths=2)\n",
    "#     ax3.set_yticks([1, 2], [\"Negative Neuron\", \"Positive Neuron\"])\n",
    "#     ax3.set_title(f\"Output Spikes | Confidence: {confidence[0]:.2f}/{confidence[1]:.2f}\")\n",
    "#     ax3.set_ylabel(\"Output Neurons\")\n",
    "\n",
    "#     # Input spikes\n",
    "#     ax4 = fig.add_subplot(gs[2, :])\n",
    "#     for i in range(2):  # up/down channels\n",
    "#         spikes = sample_data[0, i].cpu().nonzero()[:, 0].numpy()\n",
    "#         ax4.eventplot(spikes, lineoffsets=i+1, colors=['b', 'm'][i], linewidths=1)\n",
    "#     ax4.set_yticks([1, 2], [\"Input Down\", \"Input Up\"])\n",
    "#     ax4.set_xlabel(\"Time Step\")\n",
    "#     ax4.set_ylabel(\"Input Spikes\")\n",
    "\n",
    "#     plt.suptitle(f\"Sample {sample_idx} | \"\n",
    "#                 f\"True: {'Positive' if true_label else 'Negative'} | \"\n",
    "#                 f\"Predicted: {'Positive' if predicted_label else 'Negative'} | \"\n",
    "#                 f\"{'CORRECT' if true_label == predicted_label else 'WRONG'}\", \n",
    "#                 fontsize=14, y=1.02)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "#     return sample_idx\n",
    "\n",
    "# # Interactive usage:\n",
    "# def explore_samples(model, test_loader, num_steps):\n",
    "#     \"\"\"Interactive sample explorer.\"\"\"\n",
    "#     current_idx = 0\n",
    "#     total_samples = len(test_loader.dataset)\n",
    "    \n",
    "#     while True:\n",
    "#         print(f\"\\nSample {current_idx}/{total_samples-1}\")\n",
    "#         current_idx = plot_sample_spikes(model, test_loader, num_steps, current_idx)\n",
    "        \n",
    "#         user_input = input(\"Next [n], Previous [p], Jump to index [0-9], Quit [q]: \")\n",
    "#         if user_input.lower() == 'n':\n",
    "#             current_idx = (current_idx + 1) % total_samples\n",
    "#         elif user_input.lower() == 'p':\n",
    "#             current_idx = (current_idx - 1) % total_samples\n",
    "#         elif user_input.isdigit():\n",
    "#             new_idx = int(user_input)\n",
    "#             if 0 <= new_idx < total_samples:\n",
    "#                 current_idx = new_idx\n",
    "#         elif user_input.lower() == 'q':\n",
    "#             break\n",
    "\n",
    "# # Usage:\n",
    "# explore_samples(net, test_loader, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving\n",
    "# def save_model(model, optimizer, path, extra_info=None):\n",
    "#     save_dict = {\n",
    "#         'model_state_dict': model.state_dict(),\n",
    "#         'optimizer_state_dict': optimizer.state_dict(),\n",
    "#         'model_params': {\n",
    "#             'input_size': 2,\n",
    "#             'num_classes': 2,\n",
    "#             'beta': beta,\n",
    "#         }\n",
    "#     }\n",
    "#     if extra_info:\n",
    "#         save_dict.update(extra_info)\n",
    "#     torch.save(save_dict, path)\n",
    "\n",
    "# # Example usage\n",
    "# save_model(net, optimizer, 'snn_checkpoint.pth', \n",
    "#           {'loss_hist': loss_hist, 'test_acc': test_acc_hist})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lava_snn_ripples",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
