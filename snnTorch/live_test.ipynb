{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f3d6812",
   "metadata": {},
   "source": [
    "# SNN Evaluation on Real-Time Data\n",
    "In this notebook, we test the performance of trained SNNs on a real-time stream of data.\n",
    "Based (or copied) from Mesquita's [repository](https://github.com/monkin77/snn-torch/blob/master/src/hfo/5_detection/hfo_evaluation.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bb77218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pc\\Documents\\Tese\\LAVA_SNN_ripples\\snnTorch\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Show current directory\n",
    "import os\n",
    "curr_dir = os.getcwd()\n",
    "print(curr_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe62935",
   "metadata": {},
   "source": [
    "## Add Parent Directory to Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6af2449a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:\\\\nrn\\\\lib\\\\python', 'c:\\\\Users\\\\Pc\\\\anaconda3\\\\envs\\\\lava_snn_ripples\\\\python39.zip', 'c:\\\\Users\\\\Pc\\\\anaconda3\\\\envs\\\\lava_snn_ripples\\\\DLLs', 'c:\\\\Users\\\\Pc\\\\anaconda3\\\\envs\\\\lava_snn_ripples\\\\lib', 'c:\\\\Users\\\\Pc\\\\anaconda3\\\\envs\\\\lava_snn_ripples', '', 'c:\\\\Users\\\\Pc\\\\anaconda3\\\\envs\\\\lava_snn_ripples\\\\lib\\\\site-packages', 'c:\\\\Users\\\\Pc\\\\anaconda3\\\\envs\\\\lava_snn_ripples\\\\lib\\\\site-packages\\\\win32', 'c:\\\\Users\\\\Pc\\\\anaconda3\\\\envs\\\\lava_snn_ripples\\\\lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\Users\\\\Pc\\\\anaconda3\\\\envs\\\\lava_snn_ripples\\\\lib\\\\site-packages\\\\Pythonwin', 'c:\\\\Users\\\\Pc\\\\Documents\\\\Tese\\\\LAVA_SNN_ripples', 'c:\\\\Users\\\\Pc\\\\Documents\\\\Tese\\\\LAVA_SNN_ripples']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "parent_dir = os.path.abspath(os.path.join(curr_dir, os.pardir))\n",
    "# Add the grandparent directory to the system path\n",
    "# grandparent_dir = os.path.abspath(os.path.join(curr_dir, os.pardir, os.pardir))\n",
    "sys.path.append(parent_dir)\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a330e4ff",
   "metadata": {},
   "source": [
    "\n",
    "## Check if Cuda is available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9700a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Number of GPUs: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Check CUDA Installation\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# Get the number of available GPUs\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Number of GPUs: {num_gpus}\")\n",
    "\n",
    "# Get information about each GPU\n",
    "for i in range(num_gpus):\n",
    "    device_props = torch.cuda.get_device_properties(i)\n",
    "    print(f\"\\nGPU {i}:\")\n",
    "    print(f\"  Name: {device_props.name}\")\n",
    "    print(f\"  Total memory: {device_props.total_memory / 1024**3:.2f} GB\")\n",
    "    print(f\"  Multiprocessor count: {device_props.multi_processor_count}\")\n",
    "    print(f\"  Major compute capability: {device_props.major}\")\n",
    "    print(f\"  Minor compute capability: {device_props.minor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cc1aef",
   "metadata": {},
   "source": [
    "\n",
    "## Define the Device that will be used to train the SNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "595e0c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Set the device to be used\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")     # torch.device(\"cpu\") #\n",
    "\n",
    "print(\"device: \", device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc481fc8",
   "metadata": {},
   "source": [
    "\n",
    "## Define Problem and Simulation Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f97262d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Simulation Time Parameters -----\n",
    "dt = 1                         # Time between two timesteps (ms), a.k.a. virtual time step interval. (NOT ALIGNED WITH THE SAMPLING RATE OF THE INPUT DATA (2048 Hz)\n",
    "init_offset = 0 # 900 # 33400      #   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "970a052e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED_CAUSALITY_WINDOW: 5\n",
      "MAX_DETECTION_OFFSET: 72.5 ms\n"
     ]
    }
   ],
   "source": [
    "# unit: timesteps (ms) - The time window after the GT annotation where the network should predict the burst (GT_time, GT_time + PRED_CAUSALITY_WINDOW)\n",
    "# This is needed to give the network some extra time steps to increase the membrane potential and spike\n",
    "RIPPLE_DETECTION_OFFSET = [18, 45, 31, 20]\n",
    "PRED_CAUSALITY_WINDOW = int(5)     # Giving PRED_CAUSALITY_WINDOW ms for the network to update its inner state and spike  \n",
    "# in timesteps (ms) - Max time from the Insertion Timing to the GT annotation\n",
    "\n",
    "MAX_DETECTION_OFFSET = int(RIPPLE_DETECTION_OFFSET[1]) * 1.5 + PRED_CAUSALITY_WINDOW   # in timesteps (ms)\n",
    "\n",
    "print(f\"PRED_CAUSALITY_WINDOW: {PRED_CAUSALITY_WINDOW}\")\n",
    "print(f\"MAX_DETECTION_OFFSET: {MAX_DETECTION_OFFSET} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ff1dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- Network Parameters -------------------\n",
    "# We know that 2 relevant events do not occur within the confidence window of an HFO event, so we set the refractory period accordingly\n",
    "refrac_period = 200 # floor(MAX_DETECTION_OFFSET / dt)   # Number of time-steps for the refractory period\n",
    "print(f\"Refractory Period: {refrac_period} steps\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e27c7a9",
   "metadata": {},
   "source": [
    "\n",
    "## Read the Input Data and the Ground Truth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c7d7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_fs=1000\n",
    "data_dir=os.path.join(parent_dir,\"extract_Nripples\",\"train_pedro\",\"dataset_up_down\",str(downsampled_fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa63ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_data=np.load(os.path.join(data_dir,\"concat_both.npy\"),allow_pickle=True)\n",
    "ripples_GT=np.load(os.path.join(data_dir,\"ripples_both.npy\"),allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf6cd633",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'concat_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of UP spikes: \u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39msum(\u001b[43mconcat_data\u001b[49m[:, \u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of DN spikes: \u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39msum(concat_data[:, \u001b[38;5;241m1\u001b[39m]))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'concat_data' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Number of UP spikes: \", np.sum(concat_data[:, 0]))\n",
    "print(\"Number of DN spikes: \", np.sum(concat_data[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7481474",
   "metadata": {},
   "source": [
    "\n",
    "## Point of Situation\n",
    "\n",
    "Right now, we have the following:\n",
    "\n",
    "-  `concat_data` of shape (num_steps, 2) containing the UP and DN spikes.\n",
    "- `ripples_GT` of shape (num_gt_events,2) containing the timestamps (beginning and end) of every ripple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf97dc7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'concat_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Define the number of total timesteps\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m total_num_steps \u001b[38;5;241m=\u001b[39m \u001b[43mconcat_data\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      3\u001b[0m num_hfo_events \u001b[38;5;241m=\u001b[39m ripples_GT\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      4\u001b[0m num_hfo_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28msum\u001b[39m(ripples_GT[:,\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m-\u001b[39mripples_GT[:,\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'concat_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Define the number of total timesteps\n",
    "total_num_steps = concat_data.shape[0]\n",
    "num_hfo_events = ripples_GT.shape[0]\n",
    "num_hfo_timesteps=np.sum(ripples_GT[:, 1] - ripples_GT[:, 0])\n",
    "print(f\"Number of HFO Events: {num_hfo_events}\")\n",
    "print(f\"Total number of timesteps: {total_num_steps}\")\n",
    "print(\"Num of Ripple timesteps:\", num_hfo_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6221647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform ripples_GT into the time of onset...\n",
    "ripples_start=ripples_GT[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485f5fe5",
   "metadata": {},
   "source": [
    "## Create the Dataset and Dataloader to user tensor-ready data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d39ce1e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'concat_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TensorDataset, DataLoader\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Convert numpy arrays to PyTorch tensors and move them to the selected device\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[43mconcat_data\u001b[49m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      5\u001b[0m gt_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(ripples_start)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'concat_data' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors and move them to the selected device\n",
    "input_tensor = torch.from_numpy(concat_data).to(device)\n",
    "gt_tensor = torch.from_numpy(ripples_start).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4a43bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the shape of the tensors\n",
    "print(\"Input tensor shape: \", input_tensor.shape)\n",
    "print(\"GT tensor shape: \", gt_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72acba99",
   "metadata": {},
   "source": [
    "\n",
    "## Define the SNN Architecture\n",
    "Similar to what we trained before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ac85245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snntorch as snn\n",
    "import torch.nn as nn\n",
    "from snntorch import surrogate\n",
    "\n",
    "# Global Parameters\n",
    "v_thr = 1.0\n",
    "placeholder_val = 0.5\n",
    "\n",
    "# Define the surrogate gradient function to propagate spikes through the network\n",
    "spike_grad = surrogate.fast_sigmoid()   # surrogate.atan()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1f93910",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Parameters for Dense Layers\n",
    "inputDataDim = 2       # max_channel_idx - min_channel_idx + 1    # Number of input channels\n",
    "\n",
    "input_to_hidden = (inputDataDim, 24) # 16 # TODO: Increase the size of this layer # (inputDataDim, 100) # (inputDataDim, 500)  # Number of neurons in the first Fully-Connected Layer\n",
    "\n",
    "hiddenL2Dim = (input_to_hidden[1], input_to_hidden[1])  # Number of neurons in the Recurrent Fully-Connected Layer (L2)\n",
    "\n",
    "hiddenL3Dim = (input_to_hidden[1], 16)  # Number of neurons in the Fully-Connected Layer (L3)\n",
    "\n",
    "hiddenL4Dim = (hiddenL3Dim[1], input_to_hidden[1])  # Number of neurons in the Recurrent Fully-Connected Layer (L4)\n",
    "\n",
    "hidden_to_out = (hiddenL3Dim[1], 1)  # Number of neurons in the Output Fully-Connected Layer\n",
    "# In this case, we only need 1 output neuron -> Fires when HFO is detected\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4794415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize layers\n",
    "        \n",
    "        # Create a Linear Layer to serve input to LIF1\n",
    "        self.fc_in = nn.Linear(input_to_hidden[0], input_to_hidden[1],\n",
    "                bias=False,\n",
    "                dtype=torch.float32     # Set the data type of the weights to float32\n",
    "        )\n",
    "\n",
    "        # TODO: Should the LIF neurons be able to get a negative membrane potential? I think so?\n",
    "        self.lif1 = snn.Synaptic(\n",
    "            alpha=torch.full(size=(input_to_hidden[1],), fill_value=placeholder_val), \n",
    "            beta=torch.full(size=(input_to_hidden[1],), fill_value=placeholder_val),\n",
    "            threshold=v_thr,\n",
    "            reset_mechanism=\"zero\", reset_delay=False,\n",
    "            # TODO: How to add Refractory Period?\n",
    "            # init_hidden=True,   # enables the methods in snntorch.backprop to automatically clear the hidden states and detach them from the comp. graph\n",
    "            spike_grad=spike_grad,\n",
    "            learn_alpha=True,   # Learn the alpha parameter\n",
    "            learn_beta=True,    # Learn the beta parameter\n",
    "            learn_threshold=False,   # Learn the threshold parameter\n",
    "            \n",
    "        )      \n",
    "\n",
    "        \"\"\" self.fc2 = nn.Linear(\n",
    "            hiddenL2Dim[0], hiddenL2Dim[1],\n",
    "            bias=False,\n",
    "            dtype=torch.float32     # Set the data type of the weights to float32\n",
    "        ) \"\"\"\n",
    "\n",
    "        self.fc3 = nn.Linear(\n",
    "            hiddenL3Dim[0], hiddenL3Dim[1],\n",
    "            bias=False,\n",
    "            dtype=torch.float32     # Set the data type of the weights to float32\n",
    "        )\n",
    "\n",
    "        self.lif2 = snn.Synaptic(\n",
    "            alpha=torch.full(size=(hiddenL3Dim[1],), fill_value=placeholder_val), \n",
    "            beta=torch.full(size=(hiddenL3Dim[1],), fill_value=placeholder_val),\n",
    "            threshold=v_thr,\n",
    "            reset_mechanism=\"zero\", reset_delay=False,\n",
    "            # TODO: How to add Refractory Period?\n",
    "            # init_hidden=True,   # enables the methods in snntorch.backprop to automatically clear the hidden states and detach them from the comp. graph\n",
    "            spike_grad=spike_grad,\n",
    "            learn_alpha=True,   # Learn the alpha parameter\n",
    "            learn_beta=True,    # Learn the beta parameter\n",
    "            learn_threshold=False,   # Learn the threshold parameter\n",
    "        )   \n",
    "\n",
    "        \"\"\" self.fc4 = nn.Linear(\n",
    "            hiddenL4Dim[0], hiddenL4Dim[1],\n",
    "            bias=False,\n",
    "            dtype=torch.float32     # Set the data type of the weights to float32\n",
    "        ) \"\"\"\n",
    "\n",
    "        self.fc_out = nn.Linear(\n",
    "            hidden_to_out[0], hidden_to_out[1],\n",
    "            bias=False,\n",
    "            dtype=torch.float32     # Set the data type of the weights to float32\n",
    "        )\n",
    "\n",
    "        self.lif_out = snn.Synaptic(\n",
    "            alpha=placeholder_val, \n",
    "            beta=placeholder_val,\n",
    "            threshold=v_thr,\n",
    "            reset_mechanism=\"zero\", reset_delay=False,\n",
    "            # init_hidden=True,   # enables the methods in snntorch.backprop to automatically clear the hidden states and detach them from the comp. graph\n",
    "            spike_grad=spike_grad,\n",
    "            learn_alpha=True,   # Learn the alpha parameter\n",
    "            learn_beta=True,    # Learn the beta parameter\n",
    "            learn_threshold=False,   # Learn the threshold parameter\n",
    "        )\n",
    "\n",
    "        # Initialize the membrane potential of each LIF neuron\n",
    "        self.syn1, self.mem1, self.spk1 = None, None, None\n",
    "        self.syn2, self.mem2, self.spk2 = None, None, None\n",
    "        self.syn_out, self.mem_out, self.spk_out = None, None, None\n",
    "\n",
    "    \"\"\"\n",
    "    Function called during the forward pass of the network\n",
    "    \"\"\"\n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        '''\n",
    "        Forward Pass of the Network (Single Step Update)\n",
    "\n",
    "        Parameters:\n",
    "        - x: input tensor. Shape: (batch_size, num_features)\n",
    "\n",
    "        Returns:\n",
    "        - spk_vals: tuple of tensors containing the spikes of the neurons. Shape: (batch_size, num_neurons)\n",
    "        - mem_vals: tuple of tensors containing the membrane potentials of the neurons. Shape: (batch_size, num_neurons)\n",
    "        - syn_vals: tuple of tensors containing the currents of the neurons. Shape: (batch_size, num_neurons)\n",
    "        '''\n",
    "        cur_batch_size, cur_num_channels = x.shape\n",
    "\n",
    "        # --- Lazy State Initialization\n",
    "        if self.mem1 is None:\n",
    "            device = x.device   # Get the device of the input tensor\n",
    "\n",
    "            # Initialize the membrane potential of each LIF neuron\n",
    "            self.syn1, self.mem1 = self.lif1.reset_mem()\n",
    "            self.syn2, self.mem2 = self.lif2.reset_mem()\n",
    "            self.syn_out, self.mem_out = self.lif_out.reset_mem()\n",
    "\n",
    "            # Define small residual for spk1\n",
    "            spk1_factor = 0.01\n",
    "            self.spk1 = torch.rand(size=(cur_batch_size, input_to_hidden[1]), dtype=torch.float32, device=device) * spk1_factor\n",
    "            self.spk2 = torch.zeros(size=(cur_batch_size, hiddenL3Dim[1]), dtype=torch.float32, device=device) * spk1_factor\n",
    "            self.spk_out = torch.zeros(size=(cur_batch_size, hidden_to_out[1]), dtype=torch.float32, device=device)\n",
    "\n",
    "        # \n",
    "        if len(x.shape) == 1:\n",
    "            # If the input is 1D, it means we have only one feature (one channel)\n",
    "            # Unsqueeze the input to add the num_features dimension\n",
    "            x = x.unsqueeze(1)\n",
    "            \n",
    "        ############# State Update #############\n",
    "        # Calculate Input Current for LIF1 from the Input Layer (FC1) Input -> LIF1\n",
    "        cur_fc1 = self.fc_in(x) \n",
    "    \n",
    "        # Calculate Input Current from Recurrent Layer (FC2) LIF1 -> LIF1\n",
    "        # cur_fc2 = self.fc2(spk1)   # Connect LIF1 to itself using FC Layer 2 (Recurrent Layer)\n",
    "\n",
    "        # Join the input currents for LIF1 (FC1 + FC2)\n",
    "        cur1 = cur_fc1 # + cur_fc2  # TODO: Not feeding Recurent Layer to LIF1 for now\n",
    "\n",
    "        # Feed the joined input current to LIF1\n",
    "        self.spk1, self.syn1, self.mem1 = self.lif1(cur1, self.syn1, self.mem1)  # Feed input to LIF1\n",
    "\n",
    "        # Calculate Input Current for LIF2 from LIF1 (FC3) LIF1 -> LIF2\n",
    "        cur2 = self.fc3(self.spk1)   # Connect LIF1 to LIF2 using FC Layer 3\n",
    "        # Feed the input current to LIF2 and get the spikes, synaptic currents and membrane potentials\n",
    "        self.spk2, self.syn2, self.mem2 = self.lif2(cur2, self.syn2, self.mem2)  # Feed input to LIF2\n",
    "\n",
    "        # Calculate Input Current for LIF_OUT from LIF2 (FC4) LIF2 -> LIF_OUT\n",
    "        cur_out = self.fc_out(self.spk2)\n",
    "        # Feed the input current to LIF_OUT and get the spikes, synaptic currents and membrane potentials\n",
    "        self.spk_out, self.syn_out, self.mem_out = self.lif_out(cur_out, self.syn_out, self.mem_out)  # Feed input to LIF_OUT\n",
    "\n",
    "        # Return the currents, membrane potentials and spikes of the current timestep\n",
    "        syn_val = (self.syn1, self.syn2, self.syn_out)\n",
    "        mem_vals = (self.mem1, self.mem2, self.mem_out)\n",
    "        spk_vals = (self.spk1, self.spk2, self.spk_out)\n",
    "\n",
    "        # TODO: Check if the dimensions are correct\n",
    "        return (spk_vals, mem_vals, syn_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af55903",
   "metadata": {},
   "source": [
    "\n",
    "## Load the Trained Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cacbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the network onto CUDA if available\n",
    "net = Net().to(device)\n",
    "prefix=\"mesquita_test\"\n",
    "# Load the Trained Parameters from a file\n",
    "net_filename = f\"out/{prefix}_trained_net_loss.pth\"  # trained_net_loss_penalty.pth\n",
    "net.load_state_dict(torch.load(net_filename, map_location=device))\n",
    "\n",
    "# Set the network to evaluation mode\n",
    "net.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491ef421",
   "metadata": {},
   "source": [
    "## Show the Network Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6ff486b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m total_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m    \u001b[38;5;66;03m# Accumulator for the total params in the network\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Iterate through the layers of the network\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, (name, param) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mnet\u001b[49m\u001b[38;5;241m.\u001b[39mnamed_parameters()):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# print(\"param: \", param)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mSize([]):\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScalar Param (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) | Shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Value=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Display the network architecture\n",
    "total_params = 0    # Accumulator for the total params in the network\n",
    "# Iterate through the layers of the network\n",
    "for idx, (name, param) in enumerate(net.named_parameters()):\n",
    "    # print(\"param: \", param)\n",
    "    if param.shape == torch.Size([]):\n",
    "        print(f\"Scalar Param ({name}) | Shape={param.shape} | Value={param} \\n\")\n",
    "    elif len(param.shape) == 1:\n",
    "        print(f\"Vector Param ({name}) | Shape={param.shape} | Value={param} \\n\")\n",
    "    else:\n",
    "        print(f\"Tensor Param ({name}) | Shape={param.shape}. Total={param.numel()} Preview: {param[:8, :8]}\\n\")\n",
    "\n",
    "    # Add the number of parameters in the layer to the total\n",
    "    total_params += param.numel()\n",
    "\n",
    "# Print the total number of parameters in the network\n",
    "print(f\"Total Parameters: {total_params}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7e0dcd",
   "metadata": {},
   "source": [
    "## Feed the Data in Real-Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d388e813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store Confusion Matrix for Predictions\n",
    "TP, TN, FP, FN = 0, 0, 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9e0677f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lif_out_refrac_times:  tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "Tensor to store the next timestep when each output neuron may spike (after the refractory period).\n",
    "When a neuron spikes, it will be set to the current timestep + refractory period.\n",
    "\"\"\"\n",
    "lif_out_refrac_times = torch.full(size=(hidden_to_out[1],), fill_value=0.0, device=device)\n",
    "\n",
    "print(\"lif_out_refrac_times: \", lif_out_refrac_times)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fec68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_gt_idx = 0 # Index of the current GT event\n",
    "curr_gt = None  # Stores the current GT event (GT Insertion Timing)\n",
    "'''\n",
    "# Active GT Time-To-Live (TTL). This is a counter that decrements every timestep until it reaches 0.\n",
    "If it reaches 0, it means the Network failed to predict the HFO within the GT tolerance window.\n",
    "None -> No GT Event is active\n",
    "'''\n",
    "active_gt_ttl = None\n",
    "\n",
    "# Disable gradient calculation for inference\n",
    "with torch.no_grad():\n",
    "    for step in range(total_num_steps):\n",
    "        # Get the current input (UP/DN spikes)\n",
    "        curr_input = input_tensor[step]\n",
    "\n",
    "        # Unsqueeze the input to add the num_batches dimension\n",
    "        curr_input = curr_input.unsqueeze(0)\n",
    "        # print(f\"curr_input: {curr_input}\")\n",
    "\n",
    "        ADDED_FN = False    # Tracks if a False Negative was added in this timestep\n",
    "\n",
    "        # Check if a GT event leaves the detection window\n",
    "        if active_gt_ttl is not None and active_gt_ttl < 0:\n",
    "            # GT Event Expired\n",
    "            print(f\"GT Event expired at timestep {step} with GT Insertion Timing: {curr_gt}\")\n",
    "\n",
    "            # Add a False Negative to the Confusion Matrix\n",
    "            FN += 1\n",
    "            # Set the TTL to None\n",
    "            active_gt_ttl = None\n",
    "            # Move to the next GT event\n",
    "            curr_gt_idx += 1\n",
    "            # Set ADDED_FN to True\n",
    "            ADDED_FN = True\n",
    "\n",
    "        # Check if a GT event enters the detection window\n",
    "        if curr_gt_idx < num_hfo_events:\n",
    "            curr_gt = ripples_GT[curr_gt_idx,0]\n",
    "            if curr_gt == step:\n",
    "                # Check if a GT event was already active\n",
    "                if active_gt_ttl is not None:\n",
    "                    raise ValueError(\"[Error] Two GT events detected inside the Detection Window!\")\n",
    "\n",
    "                # GT Event starts at this timestep\n",
    "                # print(f\"GT Event starts at timestep {step} with GT Insertion Timing: {curr_gt}\")\n",
    "                \n",
    "                # Set the TTL to the Maximum Detection Offset from the GT Insertion Timing\n",
    "                active_gt_ttl = MAX_DETECTION_OFFSET\n",
    "            \n",
    "        # --------   State Update   --------\n",
    "        spk, mem, syn = net(curr_input)\n",
    "\n",
    "        # Get the spikes, membrane potentials and synaptic currents of the current timestep\n",
    "        spk1, spk2, spk_out = spk\n",
    "        mem1, mem2, mem_out = mem\n",
    "        syn1, syn2, syn_out = syn\n",
    "        # print(f\"spk1: {spk1.shape} | spk2: {spk2.shape} | spk_out: {spk_out.shape}\")\n",
    "        # print(f\"mem1: {mem1.shape} | mem2: {mem2.shape} | mem_out: {mem_out.shape}\")\n",
    "        # print(f\"syn1: {syn1.shape} | syn2: {syn2.shape} | syn_out: {syn_out.shape}\")\n",
    "\n",
    "        if ADDED_FN:\n",
    "            \"\"\"\n",
    "            If a FN was added -> GT event was not detected -> The problem formulation does not allow\n",
    "            2 HFO events to be closer together than the confidence window, so we can skip this step\n",
    "            \"\"\"\n",
    "            continue\n",
    "\n",
    "        \"\"\" if torch.sum(spk2) > 0:\n",
    "            # TODO: Remove this print statement\n",
    "            print(f\"LIF2 spiked at timestep {step} with spikes: {spk2}\") \"\"\"\n",
    "\n",
    "        if torch.sum(spk_out) > 0:\n",
    "            # Convert spk_out to int for bitwise operations (Squeeze the batch dimension)\n",
    "            spk_out_int = spk_out.squeeze(0).int()\n",
    "\n",
    "            # Consider the refractory period of the output neurons\n",
    "            REFRAC_STATE_MASK = lif_out_refrac_times > step * dt    # Check if each output neuron is in the refractory period\n",
    "            # Bitwise AND between the spikes by the refractory state mask\n",
    "            # Gets a mask of the neurons that spiked and are not in the refractory state\n",
    "            valid_spk_out = torch.Tensor.bool(spk_out_int & (~REFRAC_STATE_MASK))\n",
    "\n",
    "            # Check if any Valid Output Neuron spiked\n",
    "            if torch.sum(valid_spk_out) > 0:\n",
    "                # print(f\"valid_spk_out: {valid_spk_out} | lif_out_refrac_times: {lif_out_refrac_times}\")\n",
    "                # Set the refractory period for the spiking output neurons\n",
    "                lif_out_refrac_times[valid_spk_out] = float(step * dt + refrac_period)\n",
    "\n",
    "                # If an Output Neuron spiked -> Predicted an HFO\n",
    "                # Let's check if the predicted HFO is within the GT tolerance window\n",
    "                if active_gt_ttl is not None:\n",
    "                    # The GT event is active -> Valid Prediction\n",
    "                    TP += 1     # Increment True Positives\n",
    "                    active_gt_ttl = None    # Set the TTL to None\n",
    "                    curr_gt_idx += 1    # Move to the next GT event\n",
    "                    print(f\"[TP] GT Event detected at timestep {step} with GT Insertion Timing: {curr_gt}\")\n",
    "                else:\n",
    "                    # The GT event is not active -> Invalid Prediction\n",
    "                    FP += 1\n",
    "                    print(f\"[FP] Network detected HFO at timestep {step} without an active GT event\")\n",
    "            else:\n",
    "                # No valid output neuron spiked -> No HFO Detected\n",
    "                TN += 1     # Increment True Negatives (No HFO detected)\n",
    "            \n",
    "                if active_gt_ttl is not None:\n",
    "                    # Update the TTL for the active GT event\n",
    "                    active_gt_ttl -= 1\n",
    "        else:\n",
    "            # If the Output Neuron did not spike -> No HFO detected\n",
    "            TN += 1     # Increment True Negatives (No HFO detected)\n",
    "            \n",
    "            if active_gt_ttl is not None:\n",
    "                # Update the TTL for the active GT event\n",
    "                active_gt_ttl -= 1\n",
    "\n",
    "        if step % 100000 == 0:\n",
    "            print(f\"Processed step {step}/{total_num_steps} ({(step /total_num_steps) * 100:.2f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866dd9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Show the Confusion Matrix\n",
    "print(f\"Confusion Matrix:\")\n",
    "\n",
    "# Print lines with same width\n",
    "print(f\"|TP: {TP} | FP: {FP}|\\n|FN: {FN}  | TN: {TN}|\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845ec3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Calculate the performance metrics\n",
    "accuracy = round(((TP + TN) / (TP + TN + FP + FN)), 5)\n",
    "recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cadc6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Output the performance metrics\n",
    "print(f\"Accuracy (Right Prediction %): {accuracy*100:.2f} %\")\n",
    "print(f\"Recall (True Positive Rate): {recall*100:.2f} %\")\n",
    "print(f\"Specificity (True Negative Rate): {specificity*100:.2f} %\")\n",
    "print(f\"Precision (TP / (TP + FP)): {precision*100:.2f} %\")\n",
    "print(f\"F1 Score (Combines Precision & Recall): {f1_score*100:.2f} %\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b23865a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_predictions = TP + FP + TN + FN\n",
    "print(f\"Total Predictions: {total_predictions}\")\n",
    "print(f\"Total Timesteps: {total_num_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2e002f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "# Export the results to a JSON file\n",
    "OUTPUT_FOLDER = f\"eval/\"\n",
    "# create the output folder if it doesn't exist\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "# Create a dictionary with the results\n",
    "json_results = {\n",
    "    \"max_detection_offset\": MAX_DETECTION_OFFSET,\n",
    "    \"metrics\": {\n",
    "        \"true_positive\": TP,\n",
    "        \"false_positive\": FP,\n",
    "        \"true_negative\": TN,\n",
    "        \"false_negative\": FN,\n",
    "        \"total_predictions\": total_predictions,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1_score,\n",
    "        \"specificity\": specificity\n",
    "    }\n",
    "}\n",
    "\n",
    "EXPORT_JSON_FILE = True\n",
    "if EXPORT_JSON_FILE:\n",
    "    json_file_name = f\"{OUTPUT_FOLDER}/{prefix}_results_.json\"\n",
    "    with open(json_file_name, 'w') as f:\n",
    "        json.dump(json_results, f)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lava_snn_ripples",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
