{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f3d6812",
   "metadata": {},
   "source": [
    "# SNN Evaluation on Real-Time Data\n",
    "In this notebook, we test the performance of trained SNNs on a real-time stream of data.\n",
    "Based (or copied) from Mesquita's [repository](https://github.com/monkin77/snn-torch/blob/master/src/hfo/5_detection/hfo_evaluation.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7bb77218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pc\\Documents\\Tese\\LAVA_SNN_ripples\\snnTorch\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Show current directory\n",
    "import os\n",
    "curr_dir = os.getcwd()\n",
    "print(curr_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe62935",
   "metadata": {},
   "source": [
    "## Add Parent Directory to Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6af2449a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:\\\\nrn\\\\lib\\\\python', 'c:\\\\Users\\\\Pc\\\\anaconda3\\\\envs\\\\lava_snn_ripples\\\\python39.zip', 'c:\\\\Users\\\\Pc\\\\anaconda3\\\\envs\\\\lava_snn_ripples\\\\DLLs', 'c:\\\\Users\\\\Pc\\\\anaconda3\\\\envs\\\\lava_snn_ripples\\\\lib', 'c:\\\\Users\\\\Pc\\\\anaconda3\\\\envs\\\\lava_snn_ripples', '', 'c:\\\\Users\\\\Pc\\\\anaconda3\\\\envs\\\\lava_snn_ripples\\\\lib\\\\site-packages', 'c:\\\\Users\\\\Pc\\\\anaconda3\\\\envs\\\\lava_snn_ripples\\\\lib\\\\site-packages\\\\win32', 'c:\\\\Users\\\\Pc\\\\anaconda3\\\\envs\\\\lava_snn_ripples\\\\lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\Users\\\\Pc\\\\anaconda3\\\\envs\\\\lava_snn_ripples\\\\lib\\\\site-packages\\\\Pythonwin', 'c:\\\\Users\\\\Pc\\\\Documents\\\\Tese\\\\LAVA_SNN_ripples', 'c:\\\\Users\\\\Pc\\\\Documents\\\\Tese\\\\LAVA_SNN_ripples', 'c:\\\\Users\\\\Pc\\\\Documents\\\\Tese\\\\LAVA_SNN_ripples', 'c:\\\\Users\\\\Pc\\\\Documents\\\\Tese\\\\LAVA_SNN_ripples']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "parent_dir = os.path.abspath(os.path.join(curr_dir, os.pardir))\n",
    "# Add the grandparent directory to the system path\n",
    "# grandparent_dir = os.path.abspath(os.path.join(curr_dir, os.pardir, os.pardir))\n",
    "sys.path.append(parent_dir)\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a330e4ff",
   "metadata": {},
   "source": [
    "\n",
    "## Check if Cuda is available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a9700a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Number of GPUs: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Check CUDA Installation\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# Get the number of available GPUs\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Number of GPUs: {num_gpus}\")\n",
    "\n",
    "# Get information about each GPU\n",
    "for i in range(num_gpus):\n",
    "    device_props = torch.cuda.get_device_properties(i)\n",
    "    print(f\"\\nGPU {i}:\")\n",
    "    print(f\"  Name: {device_props.name}\")\n",
    "    print(f\"  Total memory: {device_props.total_memory / 1024**3:.2f} GB\")\n",
    "    print(f\"  Multiprocessor count: {device_props.multi_processor_count}\")\n",
    "    print(f\"  Major compute capability: {device_props.major}\")\n",
    "    print(f\"  Minor compute capability: {device_props.minor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cc1aef",
   "metadata": {},
   "source": [
    "\n",
    "## Define the Device that will be used to train the SNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "595e0c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Set the device to be used\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")     # torch.device(\"cpu\") #\n",
    "\n",
    "print(\"device: \", device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc481fc8",
   "metadata": {},
   "source": [
    "\n",
    "## Define Problem and Simulation Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f97262d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Simulation Time Parameters -----\n",
    "dt = 1                         # Time between two timesteps (ms), a.k.a. virtual time step interval. (NOT ALIGNED WITH THE SAMPLING RATE OF THE INPUT DATA (2048 Hz)\n",
    "init_offset = 0 # 900 # 33400      #   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "970a052e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED_CAUSALITY_WINDOW: 5\n",
      "MAX_DETECTION_OFFSET: 72.5 ms\n"
     ]
    }
   ],
   "source": [
    "# unit: timesteps (ms) - The time window after the GT annotation where the network should predict the burst (GT_time, GT_time + PRED_CAUSALITY_WINDOW)\n",
    "# This is needed to give the network some extra time steps to increase the membrane potential and spike\n",
    "RIPPLE_DETECTION_OFFSET = [18, 45, 31, 20]\n",
    "PRED_CAUSALITY_WINDOW = int(5)     # Giving PRED_CAUSALITY_WINDOW ms for the network to update its inner state and spike  \n",
    "# in timesteps (ms) - Max time from the Insertion Timing to the GT annotation\n",
    "\n",
    "MAX_DETECTION_OFFSET = int(RIPPLE_DETECTION_OFFSET[1]) * 1.5 + PRED_CAUSALITY_WINDOW   # in timesteps (ms)\n",
    "\n",
    "print(f\"PRED_CAUSALITY_WINDOW: {PRED_CAUSALITY_WINDOW}\")\n",
    "print(f\"MAX_DETECTION_OFFSET: {MAX_DETECTION_OFFSET} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "08ff1dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refractory Period: 200 steps\n"
     ]
    }
   ],
   "source": [
    "# ------------------- Network Parameters -------------------\n",
    "# We know that 2 relevant events do not occur within the confidence window of an HFO event, so we set the refractory period accordingly\n",
    "refrac_period = 200 # floor(MAX_DETECTION_OFFSET / dt)   # Number of time-steps for the refractory period\n",
    "print(f\"Refractory Period: {refrac_period} steps\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e27c7a9",
   "metadata": {},
   "source": [
    "\n",
    "## Read the Input Data and the Ground Truth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "65c7d7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_fs=1000\n",
    "data_dir=os.path.join(parent_dir,\"extract_Nripples\",\"train_pedro\",\"dataset_up_down\",str(downsampled_fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "faa63ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_data=np.load(os.path.join(data_dir,\"concat_both.npy\"),allow_pickle=True)\n",
    "ripples_GT=np.load(os.path.join(data_dir,\"ripples_both.npy\"),allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cf6cd633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of UP spikes:  2543667.0\n",
      "Number of DN spikes:  2554013.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of UP spikes: \", np.sum(concat_data[:, 0]))\n",
    "print(\"Number of DN spikes: \", np.sum(concat_data[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7481474",
   "metadata": {},
   "source": [
    "\n",
    "## Point of Situation\n",
    "\n",
    "Right now, we have the following:\n",
    "\n",
    "-  `concat_data` of shape (num_steps, 2) containing the UP and DN spikes.\n",
    "- `ripples_GT` of shape (num_gt_events,2) containing the timestamps (beginning and end) of every ripple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6cf97dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of HFO Events: 10425\n",
      "Total number of timesteps: 20284317\n",
      "Num of Ripple timesteps: 472334\n"
     ]
    }
   ],
   "source": [
    "# Define the number of total timesteps\n",
    "total_num_steps = concat_data.shape[0]\n",
    "num_hfo_events = ripples_GT.shape[0]\n",
    "num_hfo_timesteps=np.sum(ripples_GT[:, 1] - ripples_GT[:, 0])\n",
    "print(f\"Number of HFO Events: {num_hfo_events}\")\n",
    "print(f\"Total number of timesteps: {total_num_steps}\")\n",
    "print(\"Num of Ripple timesteps:\", num_hfo_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e6221647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform ripples_GT into the time of onset...\n",
    "ripples_start=ripples_GT[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485f5fe5",
   "metadata": {},
   "source": [
    "## Create the Dataset and Dataloader to user tensor-ready data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d39ce1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors and move them to the selected device\n",
    "input_tensor = torch.tensor(concat_data,dtype=torch.float32).to(device)\n",
    "gt_tensor=torch.tensor(ripples_start,dtype=torch.float32).to(device)\n",
    "# gt_tensor = torch.from_numpy(ripples_start).to(device)\n",
    "# gt_tensor = torch.from_numpy(ripples_start).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fb4a43bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor shape:  torch.Size([20284317, 2])\n",
      "GT tensor shape:  torch.Size([10425])\n"
     ]
    }
   ],
   "source": [
    "# Show the shape of the tensors\n",
    "print(\"Input tensor shape: \", input_tensor.shape)\n",
    "print(\"GT tensor shape: \", gt_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72acba99",
   "metadata": {},
   "source": [
    "\n",
    "## Define the SNN Architecture\n",
    "Similar to what we trained before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5ac85245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snntorch as snn\n",
    "import torch.nn as nn\n",
    "from snntorch import surrogate\n",
    "\n",
    "# Global Parameters\n",
    "v_thr = 1.0\n",
    "placeholder_val = 0.5\n",
    "\n",
    "# Define the surrogate gradient function to propagate spikes through the network\n",
    "spike_grad = surrogate.fast_sigmoid()   # surrogate.atan()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a1f93910",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Parameters for Dense Layers\n",
    "inputDataDim = 2       # max_channel_idx - min_channel_idx + 1    # Number of input channels\n",
    "\n",
    "input_to_hidden = (inputDataDim, 24) # 16 # TODO: Increase the size of this layer # (inputDataDim, 100) # (inputDataDim, 500)  # Number of neurons in the first Fully-Connected Layer\n",
    "\n",
    "hiddenL2Dim = (input_to_hidden[1], input_to_hidden[1])  # Number of neurons in the Recurrent Fully-Connected Layer (L2)\n",
    "\n",
    "hiddenL3Dim = (input_to_hidden[1], 16)  # Number of neurons in the Fully-Connected Layer (L3)\n",
    "\n",
    "hiddenL4Dim = (hiddenL3Dim[1], input_to_hidden[1])  # Number of neurons in the Recurrent Fully-Connected Layer (L4)\n",
    "\n",
    "hidden_to_out = (hiddenL3Dim[1], 1)  # Number of neurons in the Output Fully-Connected Layer\n",
    "# In this case, we only need 1 output neuron -> Fires when HFO is detected\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b4794415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize layers\n",
    "        \n",
    "        # Create a Linear Layer to serve input to LIF1\n",
    "        self.fc_in = nn.Linear(input_to_hidden[0], input_to_hidden[1],\n",
    "                bias=False,\n",
    "                dtype=torch.float32     # Set the data type of the weights to float32\n",
    "        )\n",
    "\n",
    "        # TODO: Should the LIF neurons be able to get a negative membrane potential? I think so?\n",
    "        self.lif1 = snn.Synaptic(\n",
    "            alpha=torch.full(size=(input_to_hidden[1],), fill_value=placeholder_val), \n",
    "            beta=torch.full(size=(input_to_hidden[1],), fill_value=placeholder_val),\n",
    "            threshold=v_thr,\n",
    "            reset_mechanism=\"zero\", reset_delay=False,\n",
    "            # TODO: How to add Refractory Period?\n",
    "            # init_hidden=True,   # enables the methods in snntorch.backprop to automatically clear the hidden states and detach them from the comp. graph\n",
    "            spike_grad=spike_grad,\n",
    "            learn_alpha=True,   # Learn the alpha parameter\n",
    "            learn_beta=True,    # Learn the beta parameter\n",
    "            learn_threshold=False,   # Learn the threshold parameter\n",
    "            \n",
    "        )      \n",
    "\n",
    "        \"\"\" self.fc2 = nn.Linear(\n",
    "            hiddenL2Dim[0], hiddenL2Dim[1],\n",
    "            bias=False,\n",
    "            dtype=torch.float32     # Set the data type of the weights to float32\n",
    "        ) \"\"\"\n",
    "\n",
    "        self.fc3 = nn.Linear(\n",
    "            hiddenL3Dim[0], hiddenL3Dim[1],\n",
    "            bias=False,\n",
    "            dtype=torch.float32     # Set the data type of the weights to float32\n",
    "        )\n",
    "\n",
    "        self.lif2 = snn.Synaptic(\n",
    "            alpha=torch.full(size=(hiddenL3Dim[1],), fill_value=placeholder_val), \n",
    "            beta=torch.full(size=(hiddenL3Dim[1],), fill_value=placeholder_val),\n",
    "            threshold=v_thr,\n",
    "            reset_mechanism=\"zero\", reset_delay=False,\n",
    "            # TODO: How to add Refractory Period?\n",
    "            # init_hidden=True,   # enables the methods in snntorch.backprop to automatically clear the hidden states and detach them from the comp. graph\n",
    "            spike_grad=spike_grad,\n",
    "            learn_alpha=True,   # Learn the alpha parameter\n",
    "            learn_beta=True,    # Learn the beta parameter\n",
    "            learn_threshold=False,   # Learn the threshold parameter\n",
    "        )   \n",
    "\n",
    "        \"\"\" self.fc4 = nn.Linear(\n",
    "            hiddenL4Dim[0], hiddenL4Dim[1],\n",
    "            bias=False,\n",
    "            dtype=torch.float32     # Set the data type of the weights to float32\n",
    "        ) \"\"\"\n",
    "\n",
    "        self.fc_out = nn.Linear(\n",
    "            hidden_to_out[0], hidden_to_out[1],\n",
    "            bias=False,\n",
    "            dtype=torch.float32     # Set the data type of the weights to float32\n",
    "        )\n",
    "\n",
    "        self.lif_out = snn.Synaptic(\n",
    "            alpha=placeholder_val, \n",
    "            beta=placeholder_val,\n",
    "            threshold=v_thr,\n",
    "            reset_mechanism=\"zero\", reset_delay=False,\n",
    "            # init_hidden=True,   # enables the methods in snntorch.backprop to automatically clear the hidden states and detach them from the comp. graph\n",
    "            spike_grad=spike_grad,\n",
    "            learn_alpha=True,   # Learn the alpha parameter\n",
    "            learn_beta=True,    # Learn the beta parameter\n",
    "            learn_threshold=False,   # Learn the threshold parameter\n",
    "        )\n",
    "\n",
    "        # Initialize the membrane potential of each LIF neuron\n",
    "        self.syn1, self.mem1, self.spk1 = None, None, None\n",
    "        self.syn2, self.mem2, self.spk2 = None, None, None\n",
    "        self.syn_out, self.mem_out, self.spk_out = None, None, None\n",
    "\n",
    "    \"\"\"\n",
    "    Function called during the forward pass of the network\n",
    "    \"\"\"\n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        '''\n",
    "        Forward Pass of the Network (Single Step Update)\n",
    "\n",
    "        Parameters:\n",
    "        - x: input tensor. Shape: (batch_size, num_features)\n",
    "\n",
    "        Returns:\n",
    "        - spk_vals: tuple of tensors containing the spikes of the neurons. Shape: (batch_size, num_neurons)\n",
    "        - mem_vals: tuple of tensors containing the membrane potentials of the neurons. Shape: (batch_size, num_neurons)\n",
    "        - syn_vals: tuple of tensors containing the currents of the neurons. Shape: (batch_size, num_neurons)\n",
    "        '''\n",
    "        cur_batch_size, cur_num_channels = x.shape\n",
    "\n",
    "        # --- Lazy State Initialization\n",
    "        if self.mem1 is None:\n",
    "            device = x.device   # Get the device of the input tensor\n",
    "\n",
    "            # Initialize the membrane potential of each LIF neuron\n",
    "            self.syn1, self.mem1 = self.lif1.reset_mem()\n",
    "            self.syn2, self.mem2 = self.lif2.reset_mem()\n",
    "            self.syn_out, self.mem_out = self.lif_out.reset_mem()\n",
    "\n",
    "            # Define small residual for spk1\n",
    "            spk1_factor = 0.01\n",
    "            self.spk1 = torch.rand(size=(cur_batch_size, input_to_hidden[1]), dtype=torch.float32, device=device) * spk1_factor\n",
    "            self.spk2 = torch.zeros(size=(cur_batch_size, hiddenL3Dim[1]), dtype=torch.float32, device=device) * spk1_factor\n",
    "            self.spk_out = torch.zeros(size=(cur_batch_size, hidden_to_out[1]), dtype=torch.float32, device=device)\n",
    "\n",
    "        # \n",
    "        if len(x.shape) == 1:\n",
    "            # If the input is 1D, it means we have only one feature (one channel)\n",
    "            # Unsqueeze the input to add the num_features dimension\n",
    "            x = x.unsqueeze(1)\n",
    "            \n",
    "        ############# State Update #############\n",
    "        # Calculate Input Current for LIF1 from the Input Layer (FC1) Input -> LIF1\n",
    "        cur_fc1 = self.fc_in(x) \n",
    "    \n",
    "        # Calculate Input Current from Recurrent Layer (FC2) LIF1 -> LIF1\n",
    "        # cur_fc2 = self.fc2(spk1)   # Connect LIF1 to itself using FC Layer 2 (Recurrent Layer)\n",
    "\n",
    "        # Join the input currents for LIF1 (FC1 + FC2)\n",
    "        cur1 = cur_fc1 # + cur_fc2  # TODO: Not feeding Recurent Layer to LIF1 for now\n",
    "\n",
    "        # Feed the joined input current to LIF1\n",
    "        self.spk1, self.syn1, self.mem1 = self.lif1(cur1, self.syn1, self.mem1)  # Feed input to LIF1\n",
    "\n",
    "        # Calculate Input Current for LIF2 from LIF1 (FC3) LIF1 -> LIF2\n",
    "        cur2 = self.fc3(self.spk1)   # Connect LIF1 to LIF2 using FC Layer 3\n",
    "        # Feed the input current to LIF2 and get the spikes, synaptic currents and membrane potentials\n",
    "        self.spk2, self.syn2, self.mem2 = self.lif2(cur2, self.syn2, self.mem2)  # Feed input to LIF2\n",
    "\n",
    "        # Calculate Input Current for LIF_OUT from LIF2 (FC4) LIF2 -> LIF_OUT\n",
    "        cur_out = self.fc_out(self.spk2)\n",
    "        # Feed the input current to LIF_OUT and get the spikes, synaptic currents and membrane potentials\n",
    "        self.spk_out, self.syn_out, self.mem_out = self.lif_out(cur_out, self.syn_out, self.mem_out)  # Feed input to LIF_OUT\n",
    "\n",
    "        # Return the currents, membrane potentials and spikes of the current timestep\n",
    "        syn_val = (self.syn1, self.syn2, self.syn_out)\n",
    "        mem_vals = (self.mem1, self.mem2, self.mem_out)\n",
    "        spk_vals = (self.spk1, self.spk2, self.spk_out)\n",
    "\n",
    "        # TODO: Check if the dimensions are correct\n",
    "        return (spk_vals, mem_vals, syn_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af55903",
   "metadata": {},
   "source": [
    "\n",
    "## Load the Trained Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a7cacbf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc_in): Linear(in_features=2, out_features=24, bias=False)\n",
       "  (lif1): Synaptic()\n",
       "  (fc3): Linear(in_features=24, out_features=16, bias=False)\n",
       "  (lif2): Synaptic()\n",
       "  (fc_out): Linear(in_features=16, out_features=1, bias=False)\n",
       "  (lif_out): Synaptic()\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the network onto CUDA if available\n",
    "net = Net().to(device)\n",
    "prefix=\"test_mesquita\"\n",
    "# Load the Trained Parameters from a file\n",
    "net_filename = f\"out/{prefix}_trained_net_loss.pth\"  # trained_net_loss_penalty.pth\n",
    "net.load_state_dict(torch.load(net_filename, map_location=device))\n",
    "\n",
    "# Set the network to evaluation mode\n",
    "net.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491ef421",
   "metadata": {},
   "source": [
    "## Show the Network Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a6ff486b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor Param (fc_in.weight) | Shape=torch.Size([24, 2]). Total=48 Preview: tensor([[ 0.0331, -0.1287],\n",
      "        [ 0.4914,  0.0858],\n",
      "        [-0.1920, -0.4826],\n",
      "        [-0.0439,  0.2868],\n",
      "        [-0.0350, -0.0056],\n",
      "        [ 0.6130, -0.0605],\n",
      "        [ 0.2599, -0.0032],\n",
      "        [ 0.0270, -0.0350]], grad_fn=<SliceBackward0>)\n",
      "\n",
      "Vector Param (lif1.beta) | Shape=torch.Size([24]) | Value=Parameter containing:\n",
      "tensor([0.4017, 0.1774, 0.7141, 0.3268, 0.4791, 0.6849, 0.7707, 0.3864, 0.0954,\n",
      "        0.3708, 0.4951, 0.8497, 0.8437, 0.4468, 0.1446, 0.3251, 0.8936, 0.4352,\n",
      "        0.5390, 0.7542, 0.6931, 0.9324, 0.6985, 0.7014], requires_grad=True) \n",
      "\n",
      "Vector Param (lif1.alpha) | Shape=torch.Size([24]) | Value=Parameter containing:\n",
      "tensor([0.3425, 0.6463, 0.7380, 0.6770, 0.8306, 0.3723, 0.2305, 0.4205, 0.2885,\n",
      "        0.1687, 0.4428, 0.8809, 0.6121, 0.4906, 0.5623, 0.6226, 0.6635, 0.3979,\n",
      "        0.3542, 0.4945, 0.4073, 0.6925, 0.4870, 0.4743], requires_grad=True) \n",
      "\n",
      "Tensor Param (fc3.weight) | Shape=torch.Size([16, 24]). Total=384 Preview: tensor([[-0.0520, -0.2070, -0.0731,  0.6094,  0.4172, -0.0165, -0.4003, -0.0950],\n",
      "        [-0.0049, -0.0580, -0.5119,  0.5941, -0.0373,  0.4027, -0.1484,  0.6959],\n",
      "        [ 0.4274, -0.0063,  0.4644, -0.3924,  0.5159, -0.1158, -0.2037, -0.0040],\n",
      "        [ 0.4699, -0.0294,  0.4504,  0.8487,  0.5676,  0.7722, -0.1530, -0.2972],\n",
      "        [-0.0079,  0.5530,  0.5221,  0.5727, -0.1607, -0.1184, -0.0806,  0.4136],\n",
      "        [ 0.5503,  0.4747,  0.4584,  0.5380,  0.9561,  0.4527, -0.1042, -0.1330],\n",
      "        [-0.1515, -0.1652, -0.2264,  0.5655,  0.6923, -0.2175, -0.1617, -0.2256],\n",
      "        [ 0.7782, -0.0400, -0.0036, -0.1374, -0.3030,  0.4621,  0.4997,  0.7032]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "Vector Param (lif2.beta) | Shape=torch.Size([16]) | Value=Parameter containing:\n",
      "tensor([0.3381, 0.6080, 0.5402, 0.3457, 0.6932, 0.5301, 0.3951, 0.6181, 0.4766,\n",
      "        0.1498, 0.5308, 0.5158, 0.0958, 0.5432, 0.3913, 0.3120],\n",
      "       requires_grad=True) \n",
      "\n",
      "Vector Param (lif2.alpha) | Shape=torch.Size([16]) | Value=Parameter containing:\n",
      "tensor([0.4497, 0.2400, 0.7399, 0.3092, 0.6070, 0.3272, 0.7496, 0.2760, 0.4132,\n",
      "        0.3456, 0.3213, 0.5296, 0.8985, 0.4372, 0.2280, 0.4354],\n",
      "       requires_grad=True) \n",
      "\n",
      "Tensor Param (fc_out.weight) | Shape=torch.Size([1, 16]). Total=16 Preview: tensor([[ 0.5482,  0.4679, -0.0798,  0.5630, -0.2153,  0.4077,  0.4833, -0.0368]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "Scalar Param (lif_out.beta) | Shape=torch.Size([]) | Value=Parameter containing:\n",
      "tensor(0.4839, requires_grad=True) \n",
      "\n",
      "Scalar Param (lif_out.alpha) | Shape=torch.Size([]) | Value=Parameter containing:\n",
      "tensor(0.4839, requires_grad=True) \n",
      "\n",
      "Total Parameters: 530\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Display the network architecture\n",
    "total_params = 0    # Accumulator for the total params in the network\n",
    "# Iterate through the layers of the network\n",
    "for idx, (name, param) in enumerate(net.named_parameters()):\n",
    "    # print(\"param: \", param)\n",
    "    if param.shape == torch.Size([]):\n",
    "        print(f\"Scalar Param ({name}) | Shape={param.shape} | Value={param} \\n\")\n",
    "    elif len(param.shape) == 1:\n",
    "        print(f\"Vector Param ({name}) | Shape={param.shape} | Value={param} \\n\")\n",
    "    else:\n",
    "        print(f\"Tensor Param ({name}) | Shape={param.shape}. Total={param.numel()} Preview: {param[:8, :8]}\\n\")\n",
    "\n",
    "    # Add the number of parameters in the layer to the total\n",
    "    total_params += param.numel()\n",
    "\n",
    "# Print the total number of parameters in the network\n",
    "print(f\"Total Parameters: {total_params}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7e0dcd",
   "metadata": {},
   "source": [
    "## Feed the Data in Real-Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d388e813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store Confusion Matrix for Predictions\n",
    "TP, TN, FP, FN = 0, 0, 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e9e0677f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lif_out_refrac_times:  tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "Tensor to store the next timestep when each output neuron may spike (after the refractory period).\n",
    "When a neuron spikes, it will be set to the current timestep + refractory period.\n",
    "\"\"\"\n",
    "lif_out_refrac_times = torch.full(size=(hidden_to_out[1],), fill_value=0.0, device=device)\n",
    "\n",
    "print(\"lif_out_refrac_times: \", lif_out_refrac_times)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fec68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed step 0/20284317 (0.00%)\n",
      "GT Event expired at timestep 1752 with GT Insertion Timing: 1679\n",
      "GT Event expired at timestep 2181 with GT Insertion Timing: 2108\n",
      "GT Event expired at timestep 2347 with GT Insertion Timing: 2274\n",
      "GT Event expired at timestep 2777 with GT Insertion Timing: 2704\n",
      "[FP] Network detected HFO at timestep 4239 without an active GT event\n",
      "[FP] Network detected HFO at timestep 7152 without an active GT event\n",
      "GT Event expired at timestep 9001 with GT Insertion Timing: 8928\n",
      "GT Event expired at timestep 9124 with GT Insertion Timing: 9051\n",
      "[FP] Network detected HFO at timestep 9247 without an active GT event\n",
      "[FP] Network detected HFO at timestep 10598 without an active GT event\n",
      "GT Event expired at timestep 10865 with GT Insertion Timing: 10792\n",
      "[TP] GT Event detected at timestep 11394 with GT Insertion Timing: 11325\n",
      "GT Event expired at timestep 11574 with GT Insertion Timing: 11501\n",
      "GT Event expired at timestep 11794 with GT Insertion Timing: 11721\n",
      "GT Event expired at timestep 12791 with GT Insertion Timing: 12718\n",
      "[FP] Network detected HFO at timestep 13023 without an active GT event\n",
      "[FP] Network detected HFO at timestep 13514 without an active GT event\n",
      "GT Event expired at timestep 16933 with GT Insertion Timing: 16860\n",
      "GT Event expired at timestep 17764 with GT Insertion Timing: 17691\n",
      "GT Event expired at timestep 17991 with GT Insertion Timing: 17918\n",
      "GT Event expired at timestep 18088 with GT Insertion Timing: 18015\n",
      "GT Event expired at timestep 18238 with GT Insertion Timing: 18165\n",
      "GT Event expired at timestep 18517 with GT Insertion Timing: 18444\n",
      "GT Event expired at timestep 18676 with GT Insertion Timing: 18603\n",
      "GT Event expired at timestep 18861 with GT Insertion Timing: 18788\n",
      "GT Event expired at timestep 19418 with GT Insertion Timing: 19345\n",
      "[FP] Network detected HFO at timestep 19768 without an active GT event\n",
      "[FP] Network detected HFO at timestep 19985 without an active GT event\n",
      "[FP] Network detected HFO at timestep 20264 without an active GT event\n",
      "GT Event expired at timestep 20462 with GT Insertion Timing: 20389\n",
      "[FP] Network detected HFO at timestep 20751 without an active GT event\n",
      "[FP] Network detected HFO at timestep 25299 without an active GT event\n",
      "[FP] Network detected HFO at timestep 27594 without an active GT event\n",
      "[FP] Network detected HFO at timestep 28043 without an active GT event\n",
      "[FP] Network detected HFO at timestep 28282 without an active GT event\n",
      "GT Event expired at timestep 30545 with GT Insertion Timing: 30472\n",
      "GT Event expired at timestep 30898 with GT Insertion Timing: 30825\n",
      "GT Event expired at timestep 31932 with GT Insertion Timing: 31859\n",
      "GT Event expired at timestep 32436 with GT Insertion Timing: 32363\n",
      "GT Event expired at timestep 33056 with GT Insertion Timing: 32983\n",
      "GT Event expired at timestep 33214 with GT Insertion Timing: 33141\n",
      "GT Event expired at timestep 34596 with GT Insertion Timing: 34523\n",
      "GT Event expired at timestep 34756 with GT Insertion Timing: 34683\n",
      "GT Event expired at timestep 35073 with GT Insertion Timing: 35000\n",
      "GT Event expired at timestep 35189 with GT Insertion Timing: 35116\n",
      "GT Event expired at timestep 37019 with GT Insertion Timing: 36946\n",
      "[FP] Network detected HFO at timestep 37360 without an active GT event\n",
      "GT Event expired at timestep 43173 with GT Insertion Timing: 43100\n",
      "GT Event expired at timestep 43336 with GT Insertion Timing: 43263\n",
      "GT Event expired at timestep 43714 with GT Insertion Timing: 43641\n",
      "GT Event expired at timestep 43880 with GT Insertion Timing: 43807\n",
      "GT Event expired at timestep 44601 with GT Insertion Timing: 44528\n",
      "[FP] Network detected HFO at timestep 44853 without an active GT event\n",
      "GT Event expired at timestep 45027 with GT Insertion Timing: 44954\n",
      "[FP] Network detected HFO at timestep 45374 without an active GT event\n",
      "GT Event expired at timestep 48730 with GT Insertion Timing: 48657\n",
      "GT Event expired at timestep 49402 with GT Insertion Timing: 49329\n",
      "GT Event expired at timestep 50147 with GT Insertion Timing: 50074\n",
      "GT Event expired at timestep 51141 with GT Insertion Timing: 51068\n",
      "GT Event expired at timestep 51376 with GT Insertion Timing: 51303\n",
      "[FP] Network detected HFO at timestep 51492 without an active GT event\n",
      "GT Event expired at timestep 51674 with GT Insertion Timing: 51601\n",
      "GT Event expired at timestep 52580 with GT Insertion Timing: 52507\n",
      "[FP] Network detected HFO at timestep 52853 without an active GT event\n",
      "GT Event expired at timestep 52964 with GT Insertion Timing: 52891\n",
      "GT Event expired at timestep 53092 with GT Insertion Timing: 53019\n",
      "GT Event expired at timestep 53250 with GT Insertion Timing: 53177\n",
      "[FP] Network detected HFO at timestep 55583 without an active GT event\n",
      "[FP] Network detected HFO at timestep 56734 without an active GT event\n",
      "[FP] Network detected HFO at timestep 57327 without an active GT event\n",
      "[FP] Network detected HFO at timestep 57528 without an active GT event\n",
      "GT Event expired at timestep 61013 with GT Insertion Timing: 60940\n",
      "GT Event expired at timestep 61164 with GT Insertion Timing: 61091\n",
      "GT Event expired at timestep 61725 with GT Insertion Timing: 61652\n",
      "GT Event expired at timestep 62148 with GT Insertion Timing: 62075\n",
      "[FP] Network detected HFO at timestep 62344 without an active GT event\n",
      "GT Event expired at timestep 62563 with GT Insertion Timing: 62490\n",
      "[FP] Network detected HFO at timestep 63718 without an active GT event\n",
      "[FP] Network detected HFO at timestep 64039 without an active GT event\n",
      "GT Event expired at timestep 66442 with GT Insertion Timing: 66369\n",
      "GT Event expired at timestep 67173 with GT Insertion Timing: 67100\n",
      "GT Event expired at timestep 67527 with GT Insertion Timing: 67454\n",
      "[FP] Network detected HFO at timestep 68183 without an active GT event\n",
      "[FP] Network detected HFO at timestep 68504 without an active GT event\n",
      "GT Event expired at timestep 71828 with GT Insertion Timing: 71755\n",
      "GT Event expired at timestep 72973 with GT Insertion Timing: 72900\n",
      "GT Event expired at timestep 73135 with GT Insertion Timing: 73062\n",
      "GT Event expired at timestep 74001 with GT Insertion Timing: 73928\n",
      "GT Event expired at timestep 75259 with GT Insertion Timing: 75186\n",
      "GT Event expired at timestep 75564 with GT Insertion Timing: 75491\n",
      "GT Event expired at timestep 76561 with GT Insertion Timing: 76488\n",
      "GT Event expired at timestep 77151 with GT Insertion Timing: 77078\n",
      "GT Event expired at timestep 80647 with GT Insertion Timing: 80574\n",
      "[FP] Network detected HFO at timestep 84228 without an active GT event\n",
      "[FP] Network detected HFO at timestep 86812 without an active GT event\n",
      "GT Event expired at timestep 87578 with GT Insertion Timing: 87505\n",
      "GT Event expired at timestep 87879 with GT Insertion Timing: 87806\n",
      "GT Event expired at timestep 88074 with GT Insertion Timing: 88001\n",
      "[FP] Network detected HFO at timestep 88534 without an active GT event\n",
      "GT Event expired at timestep 88802 with GT Insertion Timing: 88729\n",
      "GT Event expired at timestep 88882 with GT Insertion Timing: 88809\n",
      "GT Event expired at timestep 88963 with GT Insertion Timing: 88890\n",
      "[FP] Network detected HFO at timestep 89204 without an active GT event\n",
      "[FP] Network detected HFO at timestep 89727 without an active GT event\n",
      "[FP] Network detected HFO at timestep 90229 without an active GT event\n",
      "[FP] Network detected HFO at timestep 90786 without an active GT event\n",
      "[FP] Network detected HFO at timestep 91019 without an active GT event\n",
      "[FP] Network detected HFO at timestep 91752 without an active GT event\n",
      "[FP] Network detected HFO at timestep 92145 without an active GT event\n",
      "[FP] Network detected HFO at timestep 92403 without an active GT event\n",
      "[FP] Network detected HFO at timestep 93069 without an active GT event\n",
      "[FP] Network detected HFO at timestep 93914 without an active GT event\n",
      "[FP] Network detected HFO at timestep 94137 without an active GT event\n",
      "[FP] Network detected HFO at timestep 95278 without an active GT event\n",
      "[FP] Network detected HFO at timestep 95570 without an active GT event\n",
      "[FP] Network detected HFO at timestep 95844 without an active GT event\n",
      "[FP] Network detected HFO at timestep 96249 without an active GT event\n",
      "[FP] Network detected HFO at timestep 96700 without an active GT event\n",
      "[FP] Network detected HFO at timestep 96906 without an active GT event\n",
      "[FP] Network detected HFO at timestep 97108 without an active GT event\n",
      "[FP] Network detected HFO at timestep 97364 without an active GT event\n",
      "[FP] Network detected HFO at timestep 97640 without an active GT event\n",
      "[FP] Network detected HFO at timestep 98233 without an active GT event\n",
      "[FP] Network detected HFO at timestep 99671 without an active GT event\n",
      "[FP] Network detected HFO at timestep 99963 without an active GT event\n",
      "Processed step 100000/20284317 (0.49%)\n",
      "[FP] Network detected HFO at timestep 102034 without an active GT event\n",
      "[FP] Network detected HFO at timestep 103504 without an active GT event\n",
      "[FP] Network detected HFO at timestep 103815 without an active GT event\n",
      "[FP] Network detected HFO at timestep 105249 without an active GT event\n",
      "[FP] Network detected HFO at timestep 105540 without an active GT event\n",
      "[FP] Network detected HFO at timestep 106449 without an active GT event\n",
      "[FP] Network detected HFO at timestep 107829 without an active GT event\n",
      "[FP] Network detected HFO at timestep 108038 without an active GT event\n",
      "[FP] Network detected HFO at timestep 109213 without an active GT event\n",
      "[FP] Network detected HFO at timestep 112193 without an active GT event\n",
      "[FP] Network detected HFO at timestep 119408 without an active GT event\n",
      "[FP] Network detected HFO at timestep 121933 without an active GT event\n",
      "[FP] Network detected HFO at timestep 129005 without an active GT event\n",
      "[FP] Network detected HFO at timestep 134573 without an active GT event\n",
      "[FP] Network detected HFO at timestep 138004 without an active GT event\n",
      "[FP] Network detected HFO at timestep 142635 without an active GT event\n",
      "[FP] Network detected HFO at timestep 148039 without an active GT event\n",
      "[FP] Network detected HFO at timestep 152137 without an active GT event\n",
      "[FP] Network detected HFO at timestep 152383 without an active GT event\n",
      "[FP] Network detected HFO at timestep 154264 without an active GT event\n",
      "[FP] Network detected HFO at timestep 156223 without an active GT event\n",
      "[FP] Network detected HFO at timestep 156439 without an active GT event\n",
      "[FP] Network detected HFO at timestep 157820 without an active GT event\n",
      "[FP] Network detected HFO at timestep 158390 without an active GT event\n",
      "[FP] Network detected HFO at timestep 158637 without an active GT event\n",
      "[FP] Network detected HFO at timestep 159404 without an active GT event\n",
      "[FP] Network detected HFO at timestep 161912 without an active GT event\n",
      "[FP] Network detected HFO at timestep 163537 without an active GT event\n",
      "[FP] Network detected HFO at timestep 164534 without an active GT event\n",
      "[FP] Network detected HFO at timestep 164993 without an active GT event\n",
      "[FP] Network detected HFO at timestep 165199 without an active GT event\n",
      "[FP] Network detected HFO at timestep 165755 without an active GT event\n",
      "[FP] Network detected HFO at timestep 167674 without an active GT event\n",
      "[FP] Network detected HFO at timestep 172766 without an active GT event\n",
      "[FP] Network detected HFO at timestep 173157 without an active GT event\n",
      "[FP] Network detected HFO at timestep 173423 without an active GT event\n",
      "[FP] Network detected HFO at timestep 173662 without an active GT event\n",
      "[FP] Network detected HFO at timestep 175127 without an active GT event\n",
      "[FP] Network detected HFO at timestep 175709 without an active GT event\n",
      "[FP] Network detected HFO at timestep 183229 without an active GT event\n",
      "[FP] Network detected HFO at timestep 184734 without an active GT event\n",
      "[FP] Network detected HFO at timestep 185332 without an active GT event\n",
      "[FP] Network detected HFO at timestep 185564 without an active GT event\n",
      "[FP] Network detected HFO at timestep 188399 without an active GT event\n",
      "[FP] Network detected HFO at timestep 189864 without an active GT event\n",
      "[FP] Network detected HFO at timestep 190131 without an active GT event\n",
      "[FP] Network detected HFO at timestep 190379 without an active GT event\n",
      "Processed step 200000/20284317 (0.99%)\n",
      "[FP] Network detected HFO at timestep 201738 without an active GT event\n",
      "[FP] Network detected HFO at timestep 205565 without an active GT event\n",
      "[FP] Network detected HFO at timestep 205823 without an active GT event\n",
      "[FP] Network detected HFO at timestep 214469 without an active GT event\n",
      "[FP] Network detected HFO at timestep 214998 without an active GT event\n",
      "[FP] Network detected HFO at timestep 215459 without an active GT event\n",
      "[FP] Network detected HFO at timestep 216392 without an active GT event\n",
      "[FP] Network detected HFO at timestep 216947 without an active GT event\n",
      "[FP] Network detected HFO at timestep 218295 without an active GT event\n",
      "[FP] Network detected HFO at timestep 219202 without an active GT event\n",
      "[FP] Network detected HFO at timestep 220265 without an active GT event\n",
      "[FP] Network detected HFO at timestep 221104 without an active GT event\n",
      "[FP] Network detected HFO at timestep 222294 without an active GT event\n",
      "[FP] Network detected HFO at timestep 222757 without an active GT event\n",
      "[FP] Network detected HFO at timestep 224275 without an active GT event\n",
      "[FP] Network detected HFO at timestep 224826 without an active GT event\n",
      "[FP] Network detected HFO at timestep 227065 without an active GT event\n",
      "[FP] Network detected HFO at timestep 228993 without an active GT event\n",
      "[FP] Network detected HFO at timestep 229236 without an active GT event\n",
      "[FP] Network detected HFO at timestep 234673 without an active GT event\n",
      "[FP] Network detected HFO at timestep 235857 without an active GT event\n",
      "[FP] Network detected HFO at timestep 240964 without an active GT event\n",
      "[FP] Network detected HFO at timestep 242122 without an active GT event\n",
      "[FP] Network detected HFO at timestep 242496 without an active GT event\n",
      "[FP] Network detected HFO at timestep 242994 without an active GT event\n",
      "[FP] Network detected HFO at timestep 243396 without an active GT event\n",
      "[FP] Network detected HFO at timestep 243699 without an active GT event\n",
      "[FP] Network detected HFO at timestep 244698 without an active GT event\n",
      "[FP] Network detected HFO at timestep 246798 without an active GT event\n",
      "[FP] Network detected HFO at timestep 250465 without an active GT event\n",
      "[FP] Network detected HFO at timestep 252033 without an active GT event\n",
      "[FP] Network detected HFO at timestep 253291 without an active GT event\n",
      "[FP] Network detected HFO at timestep 254655 without an active GT event\n",
      "[FP] Network detected HFO at timestep 255393 without an active GT event\n",
      "[FP] Network detected HFO at timestep 256498 without an active GT event\n",
      "[FP] Network detected HFO at timestep 258164 without an active GT event\n",
      "[FP] Network detected HFO at timestep 258824 without an active GT event\n",
      "[FP] Network detected HFO at timestep 260408 without an active GT event\n",
      "[FP] Network detected HFO at timestep 262164 without an active GT event\n",
      "[FP] Network detected HFO at timestep 262645 without an active GT event\n",
      "[FP] Network detected HFO at timestep 264123 without an active GT event\n",
      "[FP] Network detected HFO at timestep 264598 without an active GT event\n",
      "[FP] Network detected HFO at timestep 264985 without an active GT event\n",
      "[FP] Network detected HFO at timestep 265232 without an active GT event\n",
      "[FP] Network detected HFO at timestep 265947 without an active GT event\n",
      "[FP] Network detected HFO at timestep 269250 without an active GT event\n",
      "[FP] Network detected HFO at timestep 269469 without an active GT event\n",
      "[FP] Network detected HFO at timestep 271021 without an active GT event\n",
      "[FP] Network detected HFO at timestep 271888 without an active GT event\n",
      "[FP] Network detected HFO at timestep 272750 without an active GT event\n",
      "[FP] Network detected HFO at timestep 273052 without an active GT event\n",
      "[FP] Network detected HFO at timestep 287287 without an active GT event\n",
      "[FP] Network detected HFO at timestep 292623 without an active GT event\n",
      "[FP] Network detected HFO at timestep 297758 without an active GT event\n",
      "Processed step 300000/20284317 (1.48%)\n",
      "[FP] Network detected HFO at timestep 304553 without an active GT event\n",
      "[FP] Network detected HFO at timestep 304769 without an active GT event\n",
      "[FP] Network detected HFO at timestep 305314 without an active GT event\n",
      "[FP] Network detected HFO at timestep 305607 without an active GT event\n",
      "[FP] Network detected HFO at timestep 307519 without an active GT event\n",
      "[FP] Network detected HFO at timestep 308357 without an active GT event\n",
      "[FP] Network detected HFO at timestep 308679 without an active GT event\n",
      "[FP] Network detected HFO at timestep 308879 without an active GT event\n",
      "[FP] Network detected HFO at timestep 309355 without an active GT event\n",
      "[FP] Network detected HFO at timestep 310454 without an active GT event\n",
      "[FP] Network detected HFO at timestep 312481 without an active GT event\n",
      "[FP] Network detected HFO at timestep 314567 without an active GT event\n",
      "[FP] Network detected HFO at timestep 315214 without an active GT event\n",
      "[FP] Network detected HFO at timestep 317145 without an active GT event\n",
      "[FP] Network detected HFO at timestep 331918 without an active GT event\n",
      "[FP] Network detected HFO at timestep 337257 without an active GT event\n",
      "[FP] Network detected HFO at timestep 337474 without an active GT event\n",
      "[FP] Network detected HFO at timestep 337732 without an active GT event\n",
      "[FP] Network detected HFO at timestep 341642 without an active GT event\n",
      "[FP] Network detected HFO at timestep 342201 without an active GT event\n",
      "[FP] Network detected HFO at timestep 342860 without an active GT event\n",
      "[FP] Network detected HFO at timestep 343439 without an active GT event\n",
      "[FP] Network detected HFO at timestep 349839 without an active GT event\n",
      "[FP] Network detected HFO at timestep 352504 without an active GT event\n",
      "[FP] Network detected HFO at timestep 353749 without an active GT event\n",
      "[FP] Network detected HFO at timestep 355508 without an active GT event\n",
      "[FP] Network detected HFO at timestep 355840 without an active GT event\n",
      "[FP] Network detected HFO at timestep 358539 without an active GT event\n",
      "[FP] Network detected HFO at timestep 358739 without an active GT event\n",
      "[FP] Network detected HFO at timestep 363964 without an active GT event\n",
      "[FP] Network detected HFO at timestep 364574 without an active GT event\n",
      "[FP] Network detected HFO at timestep 364944 without an active GT event\n",
      "[FP] Network detected HFO at timestep 367214 without an active GT event\n",
      "[FP] Network detected HFO at timestep 367455 without an active GT event\n",
      "[FP] Network detected HFO at timestep 369724 without an active GT event\n",
      "[FP] Network detected HFO at timestep 370326 without an active GT event\n",
      "[FP] Network detected HFO at timestep 376234 without an active GT event\n",
      "[FP] Network detected HFO at timestep 377165 without an active GT event\n",
      "[FP] Network detected HFO at timestep 377513 without an active GT event\n",
      "[FP] Network detected HFO at timestep 379755 without an active GT event\n",
      "[FP] Network detected HFO at timestep 380495 without an active GT event\n",
      "[FP] Network detected HFO at timestep 380730 without an active GT event\n",
      "[FP] Network detected HFO at timestep 381107 without an active GT event\n",
      "[FP] Network detected HFO at timestep 382553 without an active GT event\n",
      "[FP] Network detected HFO at timestep 382869 without an active GT event\n",
      "[FP] Network detected HFO at timestep 384029 without an active GT event\n",
      "[FP] Network detected HFO at timestep 385529 without an active GT event\n",
      "[FP] Network detected HFO at timestep 387029 without an active GT event\n",
      "[FP] Network detected HFO at timestep 388808 without an active GT event\n",
      "[FP] Network detected HFO at timestep 396694 without an active GT event\n",
      "[FP] Network detected HFO at timestep 397093 without an active GT event\n",
      "Processed step 400000/20284317 (1.97%)\n",
      "[FP] Network detected HFO at timestep 401595 without an active GT event\n",
      "[FP] Network detected HFO at timestep 402867 without an active GT event\n",
      "[FP] Network detected HFO at timestep 403223 without an active GT event\n",
      "[FP] Network detected HFO at timestep 404168 without an active GT event\n",
      "[FP] Network detected HFO at timestep 405940 without an active GT event\n",
      "[FP] Network detected HFO at timestep 422865 without an active GT event\n",
      "[FP] Network detected HFO at timestep 426420 without an active GT event\n",
      "[FP] Network detected HFO at timestep 427330 without an active GT event\n",
      "[FP] Network detected HFO at timestep 430945 without an active GT event\n",
      "[FP] Network detected HFO at timestep 437556 without an active GT event\n",
      "[FP] Network detected HFO at timestep 437779 without an active GT event\n",
      "[FP] Network detected HFO at timestep 438714 without an active GT event\n",
      "[FP] Network detected HFO at timestep 439142 without an active GT event\n",
      "[FP] Network detected HFO at timestep 440069 without an active GT event\n",
      "[FP] Network detected HFO at timestep 441809 without an active GT event\n",
      "[FP] Network detected HFO at timestep 443667 without an active GT event\n",
      "[FP] Network detected HFO at timestep 444077 without an active GT event\n",
      "[FP] Network detected HFO at timestep 444534 without an active GT event\n",
      "[FP] Network detected HFO at timestep 444831 without an active GT event\n",
      "[FP] Network detected HFO at timestep 445516 without an active GT event\n",
      "[FP] Network detected HFO at timestep 446635 without an active GT event\n",
      "[FP] Network detected HFO at timestep 446944 without an active GT event\n",
      "[FP] Network detected HFO at timestep 450935 without an active GT event\n",
      "[FP] Network detected HFO at timestep 452514 without an active GT event\n",
      "[FP] Network detected HFO at timestep 457484 without an active GT event\n",
      "[FP] Network detected HFO at timestep 463130 without an active GT event\n",
      "[FP] Network detected HFO at timestep 464906 without an active GT event\n",
      "[FP] Network detected HFO at timestep 466100 without an active GT event\n",
      "[FP] Network detected HFO at timestep 466714 without an active GT event\n",
      "[FP] Network detected HFO at timestep 467010 without an active GT event\n",
      "[FP] Network detected HFO at timestep 467575 without an active GT event\n",
      "[FP] Network detected HFO at timestep 469614 without an active GT event\n",
      "[FP] Network detected HFO at timestep 478739 without an active GT event\n",
      "[FP] Network detected HFO at timestep 481935 without an active GT event\n",
      "[FP] Network detected HFO at timestep 483569 without an active GT event\n",
      "[FP] Network detected HFO at timestep 485335 without an active GT event\n",
      "[FP] Network detected HFO at timestep 487904 without an active GT event\n",
      "[FP] Network detected HFO at timestep 489659 without an active GT event\n",
      "[FP] Network detected HFO at timestep 490477 without an active GT event\n",
      "[FP] Network detected HFO at timestep 491741 without an active GT event\n",
      "[FP] Network detected HFO at timestep 494370 without an active GT event\n",
      "[FP] Network detected HFO at timestep 496207 without an active GT event\n",
      "[FP] Network detected HFO at timestep 499169 without an active GT event\n",
      "Processed step 500000/20284317 (2.46%)\n",
      "[FP] Network detected HFO at timestep 500832 without an active GT event\n",
      "[FP] Network detected HFO at timestep 502668 without an active GT event\n",
      "[FP] Network detected HFO at timestep 504069 without an active GT event\n",
      "[FP] Network detected HFO at timestep 504333 without an active GT event\n",
      "[FP] Network detected HFO at timestep 504557 without an active GT event\n",
      "[FP] Network detected HFO at timestep 504864 without an active GT event\n",
      "[FP] Network detected HFO at timestep 505236 without an active GT event\n",
      "[FP] Network detected HFO at timestep 507856 without an active GT event\n",
      "[FP] Network detected HFO at timestep 510003 without an active GT event\n",
      "[FP] Network detected HFO at timestep 511398 without an active GT event\n",
      "[FP] Network detected HFO at timestep 516967 without an active GT event\n",
      "[FP] Network detected HFO at timestep 520299 without an active GT event\n",
      "[FP] Network detected HFO at timestep 520561 without an active GT event\n",
      "[FP] Network detected HFO at timestep 522958 without an active GT event\n",
      "[FP] Network detected HFO at timestep 524668 without an active GT event\n"
     ]
    }
   ],
   "source": [
    "curr_gt_idx = 0 # Index of the current GT event\n",
    "curr_gt = None  # Stores the current GT event (GT Insertion Timing)\n",
    "'''\n",
    "# Active GT Time-To-Live (TTL). This is a counter that decrements every timestep until it reaches 0.\n",
    "If it reaches 0, it means the Network failed to predict the HFO within the GT tolerance window.\n",
    "None -> No GT Event is active\n",
    "'''\n",
    "active_gt_ttl = None\n",
    "\n",
    "# Disable gradient calculation for inference\n",
    "with torch.no_grad():\n",
    "    for step in range(total_num_steps):\n",
    "        # Get the current input (UP/DN spikes)\n",
    "        curr_input = input_tensor[step]\n",
    "\n",
    "        # Unsqueeze the input to add the num_batches dimension\n",
    "        curr_input = curr_input.unsqueeze(0)\n",
    "        # print(f\"curr_input: {curr_input}\")\n",
    "\n",
    "        ADDED_FN = False    # Tracks if a False Negative was added in this timestep\n",
    "\n",
    "        # Check if a GT event leaves the detection window\n",
    "        if active_gt_ttl is not None and active_gt_ttl < 0:\n",
    "            # GT Event Expired\n",
    "            print(f\"GT Event expired at timestep {step} with GT Insertion Timing: {curr_gt}\")\n",
    "\n",
    "            # Add a False Negative to the Confusion Matrix\n",
    "            FN += 1\n",
    "            # Set the TTL to None\n",
    "            active_gt_ttl = None\n",
    "            # Move to the next GT event\n",
    "            curr_gt_idx += 1\n",
    "            # Set ADDED_FN to True\n",
    "            ADDED_FN = True\n",
    "\n",
    "        # Check if a GT event enters the detection window\n",
    "        if curr_gt_idx < num_hfo_events:\n",
    "            curr_gt = ripples_GT[curr_gt_idx,0]\n",
    "            if curr_gt == step:\n",
    "                # Check if a GT event was already active\n",
    "                if active_gt_ttl is not None:\n",
    "                    raise ValueError(\"[Error] Two GT events detected inside the Detection Window!\")\n",
    "\n",
    "                # GT Event starts at this timestep\n",
    "                # print(f\"GT Event starts at timestep {step} with GT Insertion Timing: {curr_gt}\")\n",
    "                \n",
    "                # Set the TTL to the Maximum Detection Offset from the GT Insertion Timing\n",
    "                active_gt_ttl = MAX_DETECTION_OFFSET\n",
    "            \n",
    "        # --------   State Update   --------\n",
    "        spk, mem, syn = net(curr_input)\n",
    "\n",
    "        # Get the spikes, membrane potentials and synaptic currents of the current timestep\n",
    "        spk1, spk2, spk_out = spk\n",
    "        mem1, mem2, mem_out = mem\n",
    "        syn1, syn2, syn_out = syn\n",
    "        # print(f\"spk1: {spk1.shape} | spk2: {spk2.shape} | spk_out: {spk_out.shape}\")\n",
    "        # print(f\"mem1: {mem1.shape} | mem2: {mem2.shape} | mem_out: {mem_out.shape}\")\n",
    "        # print(f\"syn1: {syn1.shape} | syn2: {syn2.shape} | syn_out: {syn_out.shape}\")\n",
    "\n",
    "        if ADDED_FN:\n",
    "            \"\"\"\n",
    "            If a FN was added -> GT event was not detected -> The problem formulation does not allow\n",
    "            2 HFO events to be closer together than the confidence window, so we can skip this step\n",
    "            \"\"\"\n",
    "            continue\n",
    "\n",
    "        \"\"\" if torch.sum(spk2) > 0:\n",
    "            # TODO: Remove this print statement\n",
    "            print(f\"LIF2 spiked at timestep {step} with spikes: {spk2}\") \"\"\"\n",
    "\n",
    "        if torch.sum(spk_out) > 0:\n",
    "            # Convert spk_out to int for bitwise operations (Squeeze the batch dimension)\n",
    "            spk_out_int = spk_out.squeeze(0).int()\n",
    "\n",
    "            # Consider the refractory period of the output neurons\n",
    "            REFRAC_STATE_MASK = lif_out_refrac_times > step * dt    # Check if each output neuron is in the refractory period\n",
    "            # Bitwise AND between the spikes by the refractory state mask\n",
    "            # Gets a mask of the neurons that spiked and are not in the refractory state\n",
    "            valid_spk_out = torch.Tensor.bool(spk_out_int & (~REFRAC_STATE_MASK))\n",
    "\n",
    "            # Check if any Valid Output Neuron spiked\n",
    "            if torch.sum(valid_spk_out) > 0:\n",
    "                # print(f\"valid_spk_out: {valid_spk_out} | lif_out_refrac_times: {lif_out_refrac_times}\")\n",
    "                # Set the refractory period for the spiking output neurons\n",
    "                lif_out_refrac_times[valid_spk_out] = float(step * dt + refrac_period)\n",
    "\n",
    "                # If an Output Neuron spiked -> Predicted an HFO\n",
    "                # Let's check if the predicted HFO is within the GT tolerance window\n",
    "                if active_gt_ttl is not None:\n",
    "                    # The GT event is active -> Valid Prediction\n",
    "                    TP += 1     # Increment True Positives\n",
    "                    active_gt_ttl = None    # Set the TTL to None\n",
    "                    curr_gt_idx += 1    # Move to the next GT event\n",
    "                    print(f\"[TP] GT Event detected at timestep {step} with GT Insertion Timing: {curr_gt}\")\n",
    "                else:\n",
    "                    # The GT event is not active -> Invalid Prediction\n",
    "                    FP += 1\n",
    "                    print(f\"[FP] Network detected HFO at timestep {step} without an active GT event\")\n",
    "            else:\n",
    "                # No valid output neuron spiked -> No HFO Detected\n",
    "                TN += 1     # Increment True Negatives (No HFO detected)\n",
    "            \n",
    "                if active_gt_ttl is not None:\n",
    "                    # Update the TTL for the active GT event\n",
    "                    active_gt_ttl -= 1\n",
    "        else:\n",
    "            # If the Output Neuron did not spike -> No HFO detected\n",
    "            TN += 1     # Increment True Negatives (No HFO detected)\n",
    "            \n",
    "            if active_gt_ttl is not None:\n",
    "                # Update the TTL for the active GT event\n",
    "                active_gt_ttl -= 1\n",
    "\n",
    "        if step % 100000 == 0:\n",
    "            print(f\"Processed step {step}/{total_num_steps} ({(step /total_num_steps) * 100:.2f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866dd9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Show the Confusion Matrix\n",
    "print(f\"Confusion Matrix:\")\n",
    "\n",
    "# Print lines with same width\n",
    "print(f\"|TP: {TP} | FP: {FP}|\\n|FN: {FN}  | TN: {TN}|\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845ec3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Calculate the performance metrics\n",
    "accuracy = round(((TP + TN) / (TP + TN + FP + FN)), 5)\n",
    "recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cadc6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Output the performance metrics\n",
    "print(f\"Accuracy (Right Prediction %): {accuracy*100:.2f} %\")\n",
    "print(f\"Recall (True Positive Rate): {recall*100:.2f} %\")\n",
    "print(f\"Specificity (True Negative Rate): {specificity*100:.2f} %\")\n",
    "print(f\"Precision (TP / (TP + FP)): {precision*100:.2f} %\")\n",
    "print(f\"F1 Score (Combines Precision & Recall): {f1_score*100:.2f} %\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b23865a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_predictions = TP + FP + TN + FN\n",
    "print(f\"Total Predictions: {total_predictions}\")\n",
    "print(f\"Total Timesteps: {total_num_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2e002f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "# Export the results to a JSON file\n",
    "OUTPUT_FOLDER = f\"eval/\"\n",
    "# create the output folder if it doesn't exist\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "# Create a dictionary with the results\n",
    "json_results = {\n",
    "    \"max_detection_offset\": MAX_DETECTION_OFFSET,\n",
    "    \"metrics\": {\n",
    "        \"true_positive\": TP,\n",
    "        \"false_positive\": FP,\n",
    "        \"true_negative\": TN,\n",
    "        \"false_negative\": FN,\n",
    "        \"total_predictions\": total_predictions,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1_score,\n",
    "        \"specificity\": specificity\n",
    "    }\n",
    "}\n",
    "\n",
    "EXPORT_JSON_FILE = True\n",
    "if EXPORT_JSON_FILE:\n",
    "    json_file_name = f\"{OUTPUT_FOLDER}/{prefix}_results_.json\"\n",
    "    with open(json_file_name, 'w') as f:\n",
    "        json.dump(json_results, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c5e08e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lava_snn_ripples",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
